#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Final Complete Anomaly Detection System
- Real models from models folder
- Enhanced difficulty data generation
- Model-specific folders with detailed plots
- Point/Series level metrics and confusion matrices
- Performance optimizations
- Auto GPU/DDP detection
"""

import os
import sys
import argparse
import time
import math
import warnings
import numpy as np
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.distributed as dist
import torch.multiprocessing as mp
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import Dataset, DataLoader, DistributedSampler
from torch.utils.data.distributed import DistributedSampler

# Visualization
try:
    import matplotlib
    matplotlib.use('Agg')
    import matplotlib.pyplot as plt
    import seaborn as sns
    MATPLOTLIB_AVAILABLE = True
    print("âœ… matplotlib available")
except ImportError:
    MATPLOTLIB_AVAILABLE = False
    print("âŒ matplotlib not available")

# Metrics
try:
    from sklearn.metrics import (confusion_matrix, roc_auc_score, accuracy_score, 
                                precision_score, recall_score, f1_score, roc_curve)
    SKLEARN_AVAILABLE = True
    print("âœ… sklearn available")
except ImportError:
    SKLEARN_AVAILABLE = False
    print("âŒ sklearn not available")

warnings.filterwarnings('ignore')

# ============================================================================
# ğŸ”§ GLOBAL CONFIGURATION
# ============================================================================

# GPU Environment Detection
def detect_gpu_environment():
    """GPU í™˜ê²½ ìë™ ê°ì§€ ë° ì„¤ì •"""
    if not torch.cuda.is_available():
        return {'device': 'cpu', 'world_size': 1, 'use_ddp': False}
    
    gpu_count = torch.cuda.device_count()
    print(f"ğŸ”§ GPU ê°ì§€: {gpu_count}ê°œ GPU ë°œê²¬")
    
    if gpu_count == 1:
        return {
            'device': 'cuda:0',
            'world_size': 1,
            'use_ddp': False,
            'gpu_count': gpu_count
        }
    else:
        return {
            'device': 'cuda',
            'world_size': gpu_count,
            'use_ddp': True,
            'gpu_count': gpu_count
        }

GPU_CONFIG = detect_gpu_environment()
print(f"ğŸš€ GPU Configuration: {GPU_CONFIG}")

# Configuration
CONFIG = {
    'DATA_SIZE': 1000,
    'SEQ_LEN': 64,
    'BATCH_SIZE': 16 if not GPU_CONFIG['use_ddp'] else 8,  # Adjust for DDP
    'EPOCHS': 5,
    'LEARNING_RATE': 1e-4,
    'THRESHOLD': 0.5,
    'SEED': 42,
    'WORKERS': 4
}

# Create directories
DIRS = ['samples', 'plots', 'metrics', 'confusion_matrices', 'pre_trained']
for base_dir in DIRS:
    os.makedirs(base_dir, exist_ok=True)
    
# Model-specific plot directories (updated with new ensemble model)
MODEL_NAMES = ['carla', 'tracegpt', 'patchtrad', 'prodiffad', 
               'patch_trace_ensemble', 'transfer_learning_ensemble', 
               'multi_model_ensemble', 'advanced_stacking_ensemble']

for model_name in MODEL_NAMES:
    for subdir in ['true_positive', 'true_negative', 'false_positive', 'false_negative']:
        os.makedirs(f'plots/{model_name}/{subdir}', exist_ok=True)

print(f"ğŸ“ Created {len(MODEL_NAMES)} model directories with 4 subdirectories each")

# ============================================================================
# ğŸ¯ ENHANCED DIFFICULT DATASET
# ============================================================================

class DifficultAnomalyDataset(Dataset):
    """í–¥ìƒëœ ë‚œì´ë„ì˜ ì´ìƒ íƒì§€ ë°ì´í„°ì…‹ - Normalì— ê°€ê¹Œìš´ ì´ìƒ íŒ¨í„´"""
    
    def __init__(self, mode='train', size=None, difficulty='hard'):
        np.random.seed(CONFIG['SEED'] if mode == 'train' else CONFIG['SEED'] + 1)
        
        self.mode = mode
        self.difficulty = difficulty
        data_size = size if size else (CONFIG['DATA_SIZE'] if mode == 'train' else CONFIG['DATA_SIZE'] // 4)
        
        print(f"ğŸ“Š Generating {mode} dataset: {data_size} samples (difficulty: {difficulty})")
        
        self.data = []
        self.point_labels = []
        self.series_labels = []
        self.anomaly_types = []
        self.anomaly_info = []  # ìƒì„¸ ì •ë³´
        
        # ë‚œì´ë„ë³„ ì„¤ì •
        if difficulty == 'easy':
            self.noise_std = 0.05
            self.anomaly_strength = (2.0, 4.0)
        elif difficulty == 'medium':
            self.noise_std = 0.08
            self.anomaly_strength = (1.2, 2.5)
        else:  # hard
            self.noise_std = 0.12
            self.anomaly_strength = (0.8, 1.5)  # Normalì— ë§¤ìš° ê°€ê¹Œì›€
        
        for idx in range(data_size):
            series, point_label, series_label, atype, info = self._generate_difficult_series(idx)
            self.data.append(series)
            self.point_labels.append(point_label)
            self.series_labels.append(series_label)
            self.anomaly_types.append(atype)
            self.anomaly_info.append(info)
        
        # í†µê³„ ì¶œë ¥
        normal_count = sum(1 for x in self.series_labels if x == 0)
        anomaly_count = len(self.series_labels) - normal_count
        
        type_counts = {}
        for atype in self.anomaly_types:
            type_counts[atype] = type_counts.get(atype, 0) + 1
        
        print(f"âœ… {mode} dataset complete: Normal {normal_count}, Anomaly {anomaly_count}")
        print(f"   ğŸ“Š Type distribution: {type_counts}")
    
    def _generate_difficult_series(self, idx):
        """Normalì— ê°€ê¹Œìš´ ì–´ë ¤ìš´ ì´ìƒ íŒ¨í„´ ìƒì„±"""
        series = np.random.normal(0, self.noise_std, CONFIG['SEQ_LEN']).astype(np.float32)
        point_label = np.zeros(CONFIG['SEQ_LEN'], dtype=np.float32)
        
        anomaly_type = idx % 6  # 6ê°€ì§€ íƒ€ì…
        
        if anomaly_type == 0:
            # Normal
            series_label = 0.0
            type_name = 'Normal'
            info = {'type': 'normal', 'start': 0, 'end': 0, 'magnitude': 0.0, 'count': 0}
            
        elif anomaly_type == 1:
            # Subtle Spike - ë§¤ìš° ì•½í•œ ìŠ¤íŒŒì´í¬
            n_spikes = np.random.randint(1, 3)
            spike_positions = []
            for _ in range(n_spikes):
                pos = np.random.randint(10, CONFIG['SEQ_LEN']-10)
                # ì•½í•œ ìŠ¤íŒŒì´í¬ (ê¸°ì¡´ ëŒ€ë¹„ 1/3 ê°•ë„)
                magnitude = np.random.uniform(*self.anomaly_strength) * np.random.choice([-1, 1])
                series[pos] += magnitude
                point_label[pos] = 1.0
                
                # ì£¼ë³€ ì˜í–¥ë„ ì¤„ì„
                for offset in [-1, 1]:
                    if 0 <= pos + offset < CONFIG['SEQ_LEN']:
                        series[pos + offset] += magnitude * 0.15  # ê¸°ì¡´ 0.3ì—ì„œ 0.15ë¡œ
                        point_label[pos + offset] = 1.0
                
                spike_positions.append(pos)
            
            series_label = 1.0
            type_name = 'Subtle_Spike'
            info = {'type': 'spike', 'start': spike_positions[0] if spike_positions else 0, 'end': spike_positions[-1] if spike_positions else 0, 'magnitude': magnitude, 'count': n_spikes}
            
        elif anomaly_type == 2:
            # Gradual Mean Shift - ì ì§„ì  ë³€í™”
            start = np.random.randint(15, CONFIG['SEQ_LEN']-25)
            end = start + np.random.randint(15, 25)
            
            # ì ì§„ì  ë³€í™” (ê¸‰ê²©í•˜ì§€ ì•ŠìŒ)
            shift_magnitude = np.random.uniform(*self.anomaly_strength) * np.random.choice([-1, 1])
            transition_length = min(8, (end - start) // 3)
            
            # ì‹œì‘ ë¶€ë¶„ ì ì§„ì  ì¦ê°€
            for i in range(transition_length):
                alpha = i / transition_length
                series[start + i] += shift_magnitude * alpha
                point_label[start + i] = alpha  # ê·¸ë¼ë°ì´ì…˜ ë¼ë²¨
            
            # ì¤‘ê°„ ë¶€ë¶„ ì¼ì •
            mid_start = start + transition_length
            mid_end = min(end - transition_length, CONFIG['SEQ_LEN'])
            if mid_end > mid_start:
                series[mid_start:mid_end] += shift_magnitude
                point_label[mid_start:mid_end] = 1.0
            
            # ë ë¶€ë¶„ ì ì§„ì  ê°ì†Œ
            for i in range(transition_length):
                alpha = 1 - (i / transition_length)
                series[end - transition_length + i] += shift_magnitude * alpha
                point_label[end - transition_length + i] = alpha
            
            series_label = 1.0
            type_name = 'Gradual_Shift'
            info = {'type': 'shift', 'start': start, 'end': end, 'magnitude': shift_magnitude, 'count': 1}
            
        elif anomaly_type == 3:
            # Subtle Variance Change - ë¯¸ë¬˜í•œ ë¶„ì‚° ë³€í™”
            start = np.random.randint(10, CONFIG['SEQ_LEN']-20)
            end = start + np.random.randint(15, 25)
            
            # ê¸°ì¡´ ë…¸ì´ì¦ˆ ëŒ€ë¹„ 1.5-2.5ë°° (ê¸°ì¡´ 3-6ë°°ì—ì„œ ì¤„ì„)
            new_std = self.noise_std * np.random.uniform(1.5, 2.5)
            length = end - start
            if length > 0:
                # ë°°ì—´ í¬ê¸° ì²´í¬ ë° ì•ˆì „í•œ í• ë‹¹
                actual_length = min(length, CONFIG['SEQ_LEN'] - start)
                if actual_length > 0:
                    series[start:start+actual_length] = np.random.normal(0, new_std, actual_length)
                    point_label[start:start+actual_length] = 1.0
            
            series_label = 1.0
            type_name = 'Subtle_Variance'
            info = {'type': 'variance', 'start': start, 'end': end, 'magnitude': new_std, 'count': 1}
            
        elif anomaly_type == 4:
            # Slow Trend - ì²œì²œíˆ ë³€í•˜ëŠ” íŠ¸ë Œë“œ
            start = np.random.randint(5, CONFIG['SEQ_LEN']//3)
            end = start + np.random.randint(CONFIG['SEQ_LEN']//3, CONFIG['SEQ_LEN']//2)
            end = min(end, CONFIG['SEQ_LEN'])
            
            # ì•½í•œ íŠ¸ë Œë“œ
            trend_magnitude = np.random.uniform(*self.anomaly_strength) * np.random.choice([-1, 1])
            length = end - start
            if length > 0:
                # ë°°ì—´ í¬ê¸° ì•ˆì „ ì²´í¬
                actual_length = min(length, CONFIG['SEQ_LEN'] - start)
                if actual_length > 0:
                    trend = np.linspace(0, trend_magnitude, actual_length)
                    series[start:start+actual_length] += trend
                    
                    # íŠ¸ë Œë“œ ê°•ë„ì— ë”°ë¥¸ ë¼ë²¨ (ì„ í˜• ì¦ê°€)
                    trend_labels = np.linspace(0.3, 1.0, actual_length)
                    point_label[start:start+actual_length] = trend_labels
            
            series_label = 1.0
            type_name = 'Slow_Trend'
            info = {'type': 'trend', 'start': start, 'end': end, 'magnitude': trend_magnitude, 'count': 1}
            
        else:  # anomaly_type == 5
            # Complex Pattern - ë³µí•© íŒ¨í„´ (ë§¤ìš° ì–´ë ¤ì›€)
            # ì‘ì€ ìŠ¤íŒŒì´í¬ + ë¯¸ë¬˜í•œ íŠ¸ë Œë“œ
            
            # ì‘ì€ ìŠ¤íŒŒì´í¬
            spike_pos = np.random.randint(10, CONFIG['SEQ_LEN']//2)
            spike_mag = np.random.uniform(*self.anomaly_strength) * 0.7  # ë” ì•½í•˜ê²Œ
            series[spike_pos] += spike_mag
            point_label[spike_pos] = 0.8
            
            # ë¯¸ë¬˜í•œ íŠ¸ë Œë“œ
            trend_start = CONFIG['SEQ_LEN']//2
            trend_end = CONFIG['SEQ_LEN'] - 5
            trend_mag = np.random.uniform(*self.anomaly_strength) * 0.5 * np.random.choice([-1, 1])
            trend_length = trend_end - trend_start
            if trend_length > 0:
                # ë°°ì—´ í¬ê¸° ì•ˆì „ ì²´í¬
                actual_trend_length = min(trend_length, CONFIG['SEQ_LEN'] - trend_start)
                if actual_trend_length > 0:
                    trend = np.linspace(0, trend_mag, actual_trend_length)
                    series[trend_start:trend_start+actual_trend_length] += trend
                    point_label[trend_start:trend_start+actual_trend_length] = np.linspace(0.3, 0.7, actual_trend_length)
            
            series_label = 1.0
            type_name = 'Complex_Pattern'
            info = {'type': 'complex', 'start': spike_pos, 'end': trend_end, 'magnitude': spike_mag, 'count': 2}
        
        return series, point_label, series_label, type_name, info
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        return (
            torch.from_numpy(self.data[idx]).unsqueeze(-1),  # [seq_len, 1]
            torch.from_numpy(self.point_labels[idx]),        # [seq_len]
            torch.tensor(self.series_labels[idx], dtype=torch.float32),  # scalar
            self.anomaly_types[idx],                         # string
            self.anomaly_info[idx]                          # dict
        ) 

# ============================================================================
# ğŸ¤– MODEL IMPORTS AND HYPERPARAMETERS
# ============================================================================

def safe_model_import():
    """ì•ˆì „í•œ ëª¨ë¸ import ë° fallback ì²˜ë¦¬"""
    models = {}
    
    # 1. CARLA ëª¨ë¸
    try:
        sys.path.append(os.path.join(os.path.dirname(__file__), 'models/carla'))
        from models.carla.model import create_carla_model
        models['carla'] = create_carla_model
        print("âœ… CARLA model imported successfully")
    except Exception as e:
        print(f"âŒ CARLA import failed: {e}")
        models['carla'] = None
    
    # 2. TraceGPT ëª¨ë¸
    try:
        sys.path.append(os.path.join(os.path.dirname(__file__), 'models/tracegpt'))
        from models.tracegpt.model import create_tracegpt_model
        models['tracegpt'] = create_tracegpt_model
        print("âœ… TraceGPT model imported successfully")
    except Exception as e:
        print(f"âŒ TraceGPT import failed: {e}")
        models['tracegpt'] = None
    
    # 3. PatchTrAD ëª¨ë¸
    try:
        sys.path.append(os.path.join(os.path.dirname(__file__), 'models/patchtrad'))
        from models.patchtrad.model import create_patchtrad_model
        models['patchtrad'] = create_patchtrad_model
        print("âœ… PatchTrAD model imported successfully")
    except Exception as e:
        print(f"âŒ PatchTrAD import failed: {e}")
        models['patchtrad'] = None
    
    # 4. ProDiffAD ëª¨ë¸
    try:
        sys.path.append(os.path.join(os.path.dirname(__file__), 'models/prodiffad'))
        from models.prodiffad.model import create_prodiffad_model
        models['prodiffad'] = create_prodiffad_model
        print("âœ… ProDiffAD model imported successfully")
    except Exception as e:
        print(f"âŒ ProDiffAD import failed: {e}")
        models['prodiffad'] = None
    
    return models

# Fallback ë‹¨ìˆœ ëª¨ë¸
class FallbackAnomalyModel(nn.Module):
    def __init__(self, seq_len=64, hidden_dim=128, num_layers=2, bidirectional=True, dropout=0.1, **kwargs):
        super().__init__()
        # kwargsì—ì„œ ë¬´ì‹œí•  íŒŒë¼ë¯¸í„°ë“¤ ì œê±°
        self.seq_len = seq_len
        self.hidden_dim = hidden_dim
        
        # LSTM ì¸ì½”ë”
        lstm_input_size = 1
        lstm_hidden_size = hidden_dim // 2 if bidirectional else hidden_dim
        
        self.encoder = nn.LSTM(
            lstm_input_size, 
            lstm_hidden_size, 
            num_layers, 
            batch_first=True, 
            bidirectional=bidirectional,
            dropout=dropout if num_layers > 1 else 0
        )
        
        # ë””ì½”ë”
        encoder_output_size = hidden_dim if bidirectional else lstm_hidden_size
        self.decoder = nn.Linear(encoder_output_size, 1)
        self.reconstructor = nn.Linear(encoder_output_size, seq_len)
        
    def forward(self, x):
        lstm_out, _ = self.encoder(x)
        anomaly_scores = torch.sigmoid(self.decoder(lstm_out[:, -1, :]))
        reconstruction = self.reconstructor(lstm_out[:, -1, :]).unsqueeze(-1)
        return anomaly_scores.squeeze(), reconstruction.expand_as(x)
    
    def detect_anomalies(self, x):
        with torch.no_grad():
            anomaly_scores, reconstruction = self.forward(x)
            recon_error = torch.mean((x - reconstruction) ** 2, dim=[1, 2])
            point_scores = torch.mean((x - reconstruction) ** 2, dim=2)
        return recon_error, point_scores

# ëª¨ë¸ë³„ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° - ëŒ€í­ ê°œì„  (ì›¹ê²€ìƒ‰ ê¸°ë°˜)
MODEL_HP = {
    'carla': {
        # CARLA ëª¨ë¸ ìµœì í™” (Contrastive Learning ê¸°ë°˜)
        'seq_len': CONFIG['SEQ_LEN'],
        'hidden_dim': 512,      # ë” í° representation capacity
        'encoder_layers': 4,    # ë” ê¹Šì€ ì¸ì½”ë”
        'temperature': 0.05,    # ë” ë‚®ì€ ì˜¨ë„ë¡œ ì§‘ì¤‘ì  í•™ìŠµ
        'margin': 1.5,          # ë” í° ë§ˆì§„ìœ¼ë¡œ êµ¬ë¶„ë ¥ í–¥ìƒ
        'dropout': 0.15,        # ì•½ê°„ ë†’ì€ ë“œë¡­ì•„ì›ƒìœ¼ë¡œ ê³¼ì í•© ë°©ì§€
        'contrastive_weight': 0.8,  # Contrastive loss ê°€ì¤‘ì¹˜
        'reconstruction_weight': 0.2,  # Reconstruction loss ê°€ì¤‘ì¹˜
        'lr': 3e-5,             # ë” ë‚®ì€ í•™ìŠµë¥ ë¡œ ì•ˆì •ì  í•™ìŠµ
        'epochs': 25,           # ëŒ€í­ ì¦ê°€
        'warmup_epochs': 5,     # ì›Œë°ì—… ì¶”ê°€
        'weight_decay': 1e-4
    },
    'tracegpt': {
        # TraceGPT Transformer ìµœì í™”
        'seq_len': CONFIG['SEQ_LEN'],
        'd_model': 512,         # ë” í° ëª¨ë¸ í¬ê¸°
        'n_heads': 16,          # ë” ë§ì€ ì–´í…ì…˜ í—¤ë“œ
        'n_layers': 12,         # ë” ê¹Šì€ íŠ¸ëœìŠ¤í¬ë¨¸
        'd_ff': 2048,           # ë” í° í”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬
        'dropout': 0.1,
        'attention_dropout': 0.1,
        'layer_norm_eps': 1e-6,
        'max_position_embeddings': 1024,
        'lr': 2e-5,             # íŠ¸ëœìŠ¤í¬ë¨¸ì— ì í•©í•œ ë‚®ì€ í•™ìŠµë¥ 
        'epochs': 30,           # ëŒ€í­ ì¦ê°€
        'warmup_steps': 1000,   # ì›Œë°ì—… ìŠ¤í…
        'weight_decay': 1e-4,
        'gradient_clip': 1.0
    },
    'patchtrad': {
        # PatchTrAD ìµœì í™” (Patch-based Transformer)
        'seq_len': CONFIG['SEQ_LEN'],
        'patch_size': 16,       # ë” í° íŒ¨ì¹˜ë¡œ ê¸€ë¡œë²Œ íŒ¨í„´ í¬ì°©
        'stride': 8,            # íŒ¨ì¹˜ ê°„ ì˜¤ë²„ë©
        'd_model': 512,
        'n_heads': 16,
        'n_layers': 10,
        'd_ff': 2048,
        'dropout': 0.1,
        'patch_dropout': 0.1,   # íŒ¨ì¹˜ ë“œë¡­ì•„ì›ƒ
        'positional_encoding': 'learnable',
        'lr': 1e-4,
        'epochs': 28,           # ëŒ€í­ ì¦ê°€
        'scheduler': 'cosine',   # ì½”ì‚¬ì¸ ìŠ¤ì¼€ì¤„ëŸ¬
        'weight_decay': 1e-4
    },
    'prodiffad': {
        # ProDiffAD Diffusion ëª¨ë¸ ìµœì í™”
        'seq_len': CONFIG['SEQ_LEN'],
        'hidden_dim': 512,
        'num_layers': 8,        # ë” ê¹Šì€ ë„¤íŠ¸ì›Œí¬
        'dropout': 0.1,
        'num_timesteps': 1000,  # Diffusion timesteps
        'beta_schedule': 'cosine',  # ë² íƒ€ ìŠ¤ì¼€ì¤„
        'denoising_steps': 50,
        'lr': 1e-4,
        'epochs': 35,           # Diffusionì€ ë” ë§ì€ epochs í•„ìš”
        'ema_decay': 0.9999,    # Exponential Moving Average
        'weight_decay': 1e-5
    },
    'fallback': {
        # í–¥ìƒëœ Fallback ëª¨ë¸
        'seq_len': CONFIG['SEQ_LEN'],
        'hidden_dim': 256,      # ë” í° í¬ê¸°
        'num_layers': 3,        # ë” ê¹Šì€ LSTM
        'bidirectional': True,  # ì–‘ë°©í–¥ LSTM
        'dropout': 0.2,
        'lr': 5e-4,             # ì•½ê°„ ë‚®ì¶˜ í•™ìŠµë¥ 
        'epochs': 20,           # ì¦ê°€
        'scheduler': 'step',
        'step_size': 10,
        'gamma': 0.5
    }
}

# í–¥ìƒëœ ì•™ìƒë¸” ëª¨ë¸ë“¤ - Stacking ê¸°ë°˜ ê°œì„ 
class AdvancedStackingEnsemble(nn.Module):
    """ê³ ê¸‰ ìŠ¤íƒœí‚¹ ì•™ìƒë¸” - ë©”íƒ€ í•™ìŠµê¸° í¬í•¨"""
    def __init__(self, models_dict, device='cpu'):
        super().__init__()
        self.models = {k: v for k, v in models_dict.items() if v is not None and 'ensemble' not in k}
        self.device = device
        
        # ë©”íƒ€ í•™ìŠµê¸° (2ì¸µ ì‹ ê²½ë§)
        meta_input_size = len(self.models) * 2  # series + point scores
        self.meta_learner = nn.Sequential(
            nn.Linear(meta_input_size, 64),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(32, 2)  # series, point outputs
        ).to(device)
        
        # ëª¨ë¸ë³„ ê°€ì¤‘ì¹˜
        self.model_weights = nn.Parameter(torch.ones(len(self.models)))
        
    def forward(self, x):
        """í›ˆë ¨ ì‹œ ì‚¬ìš©"""
        if not self.models:
            return torch.zeros(x.shape[0], 2, device=self.device)
        
        # ëª¨ë“  ëª¨ë¸ì˜ ì˜ˆì¸¡ ìˆ˜ì§‘
        predictions = []
        for model in self.models.values():
            series, point = model.detect_anomalies(x)
            # ê° ì‹œí€€ìŠ¤ì˜ í‰ê·  point score
            point_avg = torch.mean(point, dim=1) if len(point.shape) > 1 else point
            predictions.extend([series, point_avg])
        
        # ë©”íƒ€ í•™ìŠµê¸° ì…ë ¥
        meta_input = torch.stack(predictions, dim=1)
        meta_output = self.meta_learner(meta_input)
        
        return meta_output
    
    def detect_anomalies(self, x):
        """ì¶”ë¡  ì‹œ ì‚¬ìš©"""
        with torch.no_grad():
            meta_output = self.forward(x)
            series_scores = torch.sigmoid(meta_output[:, 0])
            point_scores = torch.sigmoid(meta_output[:, 1]).unsqueeze(1).expand(-1, x.shape[1])
            
            return series_scores, point_scores

class PatchTraceEnsemble(nn.Module):
    """í–¥ìƒëœ PatchTrAD + TraceGPT ì•™ìƒë¸” - ì–´í…ì…˜ ê¸°ë°˜ ê°€ì¤‘ì¹˜"""
    def __init__(self, models_dict, device='cpu'):
        super().__init__()
        self.patch_model = models_dict.get('patchtrad')
        self.trace_model = models_dict.get('tracegpt')
        self.device = device
        
        # ë™ì  ê°€ì¤‘ì¹˜ ê³„ì‚°ì„ ìœ„í•œ ì–´í…ì…˜ ëª¨ë“ˆ
        self.attention = nn.Sequential(
            nn.Linear(2, 16),  # 2ê°œ ëª¨ë¸ ìŠ¤ì½”ì–´
            nn.ReLU(),
            nn.Linear(16, 2),  # ê°€ì¤‘ì¹˜ ì¶œë ¥
            nn.Softmax(dim=-1)
        ).to(device)
        
        # ì„±ëŠ¥ ê¸°ë°˜ ì´ˆê¸° ê°€ì¤‘ì¹˜ (PatchTrADê°€ ì¼ë°˜ì ìœ¼ë¡œ ë” ì•ˆì •ì )
        self.base_weights = torch.tensor([0.6, 0.4], device=device)
        
    def detect_anomalies(self, x):
        if self.patch_model and self.trace_model:
            with torch.no_grad():
                patch_series, patch_point = self.patch_model.detect_anomalies(x)
                trace_series, trace_point = self.trace_model.detect_anomalies(x)
                
                # í‰ê·  ìŠ¤ì½”ì–´ë¡œ ë™ì  ê°€ì¤‘ì¹˜ ê³„ì‚°
                avg_scores = torch.stack([
                    torch.mean(patch_series),
                    torch.mean(trace_series)
                ]).unsqueeze(0)
                
                dynamic_weights = self.attention(avg_scores).squeeze(0)
                
                # ê¸°ë³¸ ê°€ì¤‘ì¹˜ì™€ ë™ì  ê°€ì¤‘ì¹˜ ê²°í•©
                final_weights = 0.7 * self.base_weights + 0.3 * dynamic_weights
                
                combined_series = final_weights[0] * patch_series + final_weights[1] * trace_series
                combined_point = final_weights[0] * patch_point + final_weights[1] * trace_point
                
                return combined_series, combined_point
        return torch.zeros(x.shape[0], device=self.device), torch.zeros(x.shape[0], x.shape[1], device=self.device)

class TransferLearningEnsemble(nn.Module):
    """í–¥ìƒëœ ì „ì´í•™ìŠµ ê¸°ë°˜ ì•™ìƒë¸” - CARLA íŠ¹ì„± í™œìš©"""
    def __init__(self, models_dict, device='cpu'):
        super().__init__()
        self.carla_model = models_dict.get('carla')
        self.trace_model = models_dict.get('tracegpt')
        self.device = device
        
        # CARLAì˜ contrastive learning íŠ¹ì„±ì„ í™œìš©í•œ confidence ê³„ì‚°
        self.confidence_net = nn.Sequential(
            nn.Linear(2, 8),
            nn.ReLU(),
            nn.Linear(8, 1),
            nn.Sigmoid()
        ).to(device)
        
    def detect_anomalies(self, x):
        if self.carla_model and self.trace_model:
            with torch.no_grad():
                carla_series, carla_point = self.carla_model.detect_anomalies(x)
                trace_series, trace_point = self.trace_model.detect_anomalies(x)
                
                # Confidence score ê³„ì‚° (CARLAê°€ ë” í™•ì‹ í•  ë•Œ ê°€ì¤‘ì¹˜ ì¦ê°€)
                confidence_input = torch.stack([
                    torch.mean(carla_series),
                    torch.var(carla_series)  # ë¶„ì‚°ì´ ë‚®ìœ¼ë©´ ë” í™•ì‹ 
                ]).unsqueeze(0)
                
                carla_confidence = self.confidence_net(confidence_input).item()
                
                # ë™ì  ê°€ì¤‘ì¹˜ (CARLA í™•ì‹ ë„ì— ë”°ë¼ ì¡°ì •)
                carla_weight = 0.6 + 0.3 * carla_confidence  # 0.6~0.9
                trace_weight = 1.0 - carla_weight
                
                combined_series = carla_weight * carla_series + trace_weight * trace_series
                combined_point = carla_weight * carla_point + trace_weight * trace_point
                
                return combined_series, combined_point
        return torch.zeros(x.shape[0], device=self.device), torch.zeros(x.shape[1], device=self.device)

class MultiModelEnsemble(nn.Module):
    """í–¥ìƒëœ ì „ì²´ ëª¨ë¸ ì•™ìƒë¸” - ë‹¤ì–‘ì„±ê³¼ ì •í™•ë„ ê· í˜•"""
    def __init__(self, models_dict, device='cpu'):
        super().__init__()
        self.models = {k: v for k, v in models_dict.items() if v is not None and 'ensemble' not in k}
        self.device = device
        
        if self.models:
            # ëª¨ë¸ë³„ ì„±ëŠ¥ ê¸°ë°˜ ì´ˆê¸° ê°€ì¤‘ì¹˜
            performance_weights = {
                'carla': 0.25,      # Contrastive learning ìš°ìˆ˜
                'tracegpt': 0.3,    # Transformer ê°•ë ¥í•¨
                'patchtrad': 0.25,  # Patch ê¸°ë°˜ ì•ˆì •ì 
                'prodiffad': 0.2    # Diffusion ëª¨ë¸ í˜ì‹ ì 
            }
            
            weights = [performance_weights.get(k, 1.0) for k in self.models.keys()]
            self.weights = nn.Parameter(torch.tensor(weights, device=device))
            
            # ë‹¤ì–‘ì„± ë³´ìƒ ë©”ì»¤ë‹ˆì¦˜
            self.diversity_factor = nn.Parameter(torch.tensor(0.1, device=device))
            
    def detect_anomalies(self, x):
        if not self.models:
            return torch.zeros(x.shape[0], device=self.device), torch.zeros(x.shape[0], x.shape[1], device=self.device)
            
        with torch.no_grad():
            series_scores = []
            point_scores = []
            
            for model in self.models.values():
                series, point = model.detect_anomalies(x)
                series_scores.append(series)
                point_scores.append(point)
            
            # ì •ê·œí™”ëœ ê°€ì¤‘ì¹˜
            weights = torch.softmax(self.weights, dim=0)
            
            # ê¸°ë³¸ ê°€ì¤‘ í‰ê· 
            combined_series = sum(w * s for w, s in zip(weights, series_scores))
            combined_point = sum(w * p for w, p in zip(weights, point_scores))
            
            # ë‹¤ì–‘ì„± ë³´ìƒ: ëª¨ë¸ ê°„ ë¶ˆì¼ì¹˜ê°€ í´ ë•Œ ì¡°ì •
            if len(series_scores) > 1:
                series_var = torch.var(torch.stack(series_scores), dim=0)
                diversity_bonus = torch.tanh(self.diversity_factor * series_var)
                combined_series = combined_series + diversity_bonus
                combined_series = torch.clamp(combined_series, 0, 1)
            
            return combined_series, combined_point

# ëª¨ë¸ ìƒì„± í•¨ìˆ˜
def create_all_models():
    """ëª¨ë“  ëª¨ë¸ ìƒì„± ë° ì„¤ì •"""
    print("ğŸš€ Creating all models...")
    
    # Import models
    imported_models = safe_model_import()
    models = {}
    
    # ê°œë³„ ëª¨ë¸ë“¤
    for name, create_func in imported_models.items():
        # ëª¨ë¸ ìƒì„±ìš© ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° (í›ˆë ¨ ê´€ë ¨ ì œì™¸)
        basic_params = ['seq_len', 'hidden_dim', 'd_model', 'n_heads', 'n_layers', 'd_ff', 'dropout',
                       'patch_size', 'stride', 'encoder_layers', 'temperature', 'margin', 'num_layers']
        
        model_params = {k: v for k, v in MODEL_HP[name].items() 
                       if k in basic_params and k not in ['lr', 'epochs']}
        
        fallback_params = {k: v for k, v in MODEL_HP['fallback'].items() 
                          if k in basic_params and k not in ['lr', 'epochs']}
        
        if create_func is not None:
            try:
                print(f"ğŸ”§ Creating {name.upper()} with params: {model_params}")
                model = create_func(**model_params)
                if GPU_CONFIG['device'] != 'cpu':
                    model = model.to(GPU_CONFIG['device'])
                models[name] = model
                print(f"âœ… {name.upper()} model created successfully")
            except Exception as e:
                print(f"âŒ {name.upper()} model creation failed: {e}")
                # Fallback ëª¨ë¸ ì‚¬ìš©
                print(f"ğŸ”„ Creating fallback model with params: {fallback_params}")
                model = FallbackAnomalyModel(**fallback_params)
                if GPU_CONFIG['device'] != 'cpu':
                    model = model.to(GPU_CONFIG['device'])
                models[name] = model
                print(f"ğŸ”„ Using fallback model for {name.upper()}")
        else:
            # Fallback ëª¨ë¸
            print(f"ğŸ”„ Creating fallback model for {name.upper()} with params: {fallback_params}")
            model = FallbackAnomalyModel(**fallback_params)
            if GPU_CONFIG['device'] != 'cpu':
                model = model.to(GPU_CONFIG['device'])
            models[name] = model
            print(f"ğŸ”„ Using fallback model for {name.upper()}")
    
    # í–¥ìƒëœ ì•™ìƒë¸” ëª¨ë¸ë“¤
    device = GPU_CONFIG['device']
    models['patch_trace_ensemble'] = PatchTraceEnsemble(models, device)
    models['transfer_learning_ensemble'] = TransferLearningEnsemble(models, device)
    models['multi_model_ensemble'] = MultiModelEnsemble(models, device)
    models['advanced_stacking_ensemble'] = AdvancedStackingEnsemble(models, device)
    
    if GPU_CONFIG['device'] != 'cpu':
        ensemble_names = ['patch_trace_ensemble', 'transfer_learning_ensemble', 
                         'multi_model_ensemble', 'advanced_stacking_ensemble']
        for name in ensemble_names:
            models[name] = models[name].to(GPU_CONFIG['device'])
    
    print(f"ğŸ¯ Total {len(models)} models created (including 4 ensemble models)")
    return models 

# ============================================================================
# ğŸ“Š DETAILED VISUALIZATION AND EVALUATION
# ============================================================================

def create_detailed_plot(data, point_true, point_pred, series_true, series_pred, 
                        threshold, model_name, sample_idx, anomaly_type, save_dir):
    """ê°œì„ ëœ ìƒì„¸ ì‹œê³„ì—´ í”Œë¡¯ ìƒì„± - ì‚¬ìš©ì ìš”ì²­ì‚¬í•­ ë°˜ì˜"""
    if not MATPLOTLIB_AVAILABLE:
        return
        
    # 2x1 ì„œë¸Œí”Œë¡¯ (3ë²ˆì§¸ í”Œë¡¯ì„ 2ë²ˆì§¸ì™€ í•©ì¹¨)
    fig, axes = plt.subplots(2, 1, figsize=(20, 16))
    
    # Time axis
    time_axis = np.arange(len(data))
    
    # 1. Signal + True/Pred Anomalies + Anomaly Zone
    axes[0].plot(time_axis, data, 'b-', linewidth=4, label='Signal', alpha=0.9)
    
    # True anomalies - ë¹¨ê°„ìƒ‰ ì›
    true_anomaly_mask = point_true > 0.5
    if np.any(true_anomaly_mask):
        axes[0].scatter(time_axis[true_anomaly_mask], data[true_anomaly_mask], 
                       color='red', s=120, label='True Anomalies', alpha=0.9, 
                       zorder=5, edgecolors='darkred', linewidth=2)
    
    # Predicted anomalies - ì˜¤ë Œì§€ìƒ‰ ì‚¼ê°í˜•
    pred_anomaly_mask = point_pred > threshold
    if np.any(pred_anomaly_mask):
        axes[0].scatter(time_axis[pred_anomaly_mask], data[pred_anomaly_mask], 
                       color='orange', s=80, marker='^', label='Pred Anomalies', 
                       alpha=0.8, zorder=4, edgecolors='darkorange', linewidth=2)
    
    # Anomaly zone highlighting
    axes[0].fill_between(time_axis, data.min(), data.max(), 
                        where=(point_pred > threshold), alpha=0.25, color='red', 
                        label='Anomaly Zone')
    
    axes[0].set_title(f'{model_name} - Sample {sample_idx} ({anomaly_type})\n'
                     f'Series: True={series_true:.1f}, Pred={series_pred:.3f}', 
                     fontsize=24, fontweight='bold', pad=20)
    axes[0].set_ylabel('Signal Value', fontsize=20, fontweight='bold')
    axes[0].legend(fontsize=18, loc='upper right')
    axes[0].grid(True, alpha=0.4, linewidth=1.5)
    axes[0].tick_params(labelsize=18)
    
    # 2. Anomaly Score + True Labels + Threshold (2ë²ˆì§¸ì™€ 3ë²ˆì§¸ í•©ì¹¨)
    # Twin axis for better visualization
    ax2_twin = axes[1].twinx()
    
    # Anomaly score - ë…¹ìƒ‰
    line1 = axes[1].plot(time_axis, point_pred, 'g-', linewidth=4, 
                        label='Anomaly Score', alpha=0.9)
    axes[1].axhline(y=threshold, color='red', linestyle='--', linewidth=4, 
                   label=f'Threshold ({threshold:.3f})', alpha=0.9)
    
    # ì„ê³„ê°’ì„ ë„˜ëŠ” ì˜ì—­ ê°•ì¡°
    axes[1].fill_between(time_axis, point_pred, threshold, 
                        where=(point_pred > threshold), alpha=0.4, 
                        color='red', label='Anomaly Zone')
    
    # True labels - ë³´ë¼ìƒ‰ (ìš°ì¸¡ ì¶•)
    line2 = ax2_twin.plot(time_axis, point_true, color='purple', linewidth=4, 
                         label='True Labels', alpha=0.9, linestyle='-.')
    
    axes[1].set_title('Anomaly Score vs True Labels with Threshold', 
                     fontsize=24, fontweight='bold', pad=20)
    axes[1].set_xlabel('Time Steps', fontsize=20, fontweight='bold')
    axes[1].set_ylabel('Anomaly Score', fontsize=20, fontweight='bold', color='green')
    ax2_twin.set_ylabel('True Labels (0=Normal, 1=Anomaly)', 
                       fontsize=20, fontweight='bold', color='purple')
    
    # Yì¶• ë²”ìœ„ ì„¤ì •
    axes[1].set_ylim(-0.05, 1.05)
    ax2_twin.set_ylim(-0.05, 1.05)
    
    # ë²”ë¡€ í•©ì¹˜ê¸°
    lines1, labels1 = axes[1].get_legend_handles_labels()
    lines2, labels2 = ax2_twin.get_legend_handles_labels()
    axes[1].legend(lines1 + lines2, labels1 + labels2, 
                  fontsize=18, loc='upper left')
    
    axes[1].grid(True, alpha=0.4, linewidth=1.5)
    axes[1].tick_params(labelsize=18, colors='green')
    ax2_twin.tick_params(labelsize=18, colors='purple')
    
    plt.tight_layout()
    
    # Save plot
    os.makedirs(save_dir, exist_ok=True)
    filename = f'{save_dir}/sample_{sample_idx}_{anomaly_type}.png'
    plt.savefig(filename, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    return filename

def create_confusion_matrix_plot(true_labels, pred_labels, model_name, level, save_dir):
    """í˜¼ë™ í–‰ë ¬ ìƒì„±"""
    if not SKLEARN_AVAILABLE or not MATPLOTLIB_AVAILABLE:
        return None
        
    # ì´ì§„ ë¶„ë¥˜ë¡œ ë³€í™˜
    true_binary = (true_labels > 0.5).astype(int)
    pred_binary = (pred_labels > 0.5).astype(int)
    
    cm = confusion_matrix(true_binary, pred_binary)
    
    fig, ax = plt.subplots(figsize=(8, 6))
    im = ax.imshow(cm, interpolation='nearest', cmap='Blues')
    ax.figure.colorbar(im, ax=ax)
    
    # í´ë˜ìŠ¤ ì´ë¦„
    classes = ['Normal', 'Anomaly']
    ax.set(xticks=np.arange(cm.shape[1]),
           yticks=np.arange(cm.shape[0]),
           xticklabels=classes, yticklabels=classes,
           title=f'{model_name} - {level} Level Confusion Matrix',
           ylabel='True Label',
           xlabel='Predicted Label')
    
    # í…ìŠ¤íŠ¸ ì£¼ì„
    thresh = cm.max() / 2.
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j, i, format(cm[i, j], 'd'),
                   ha="center", va="center",
                   color="white" if cm[i, j] > thresh else "black")
    
    plt.tight_layout()
    
    os.makedirs(save_dir, exist_ok=True)
    filename = f'{save_dir}/{model_name}_{level}_confusion_matrix.png'
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    plt.close()
    
    return filename

def calculate_detailed_metrics(true_labels, pred_scores, threshold=0.5):
    """ìƒì„¸ ë©”íŠ¸ë¦­ ê³„ì‚°"""
    if not SKLEARN_AVAILABLE:
        return {}
        
    # ì´ì§„ ì˜ˆì¸¡ê°’
    pred_binary = (pred_scores > threshold).astype(int)
    true_binary = (true_labels > 0.5).astype(int)
    
    metrics = {
        'accuracy': accuracy_score(true_binary, pred_binary),
        'precision': precision_score(true_binary, pred_binary, zero_division='warn'),
        'recall': recall_score(true_binary, pred_binary, zero_division='warn'),
        'f1': f1_score(true_binary, pred_binary, zero_division='warn'),
        'threshold': threshold
    }
    
    # AUC ê³„ì‚° (ê°€ëŠ¥í•œ ê²½ìš°)
    try:
        metrics['auc'] = roc_auc_score(true_binary, pred_scores)
    except:
        metrics['auc'] = 0.0
        
    return metrics

def save_sample_data(dataset, model_name, num_samples=10):
    """ìƒ˜í”Œ ë°ì´í„° ì €ì¥"""
    if not MATPLOTLIB_AVAILABLE:
        return
        
    fig, axes = plt.subplots(2, 5, figsize=(20, 8))
    axes = axes.flatten()
    
    for i in range(min(num_samples, len(dataset))):
        data, point_labels, series_label, anomaly_type, info = dataset[i]
        
        ax = axes[i]
        time_axis = np.arange(len(data))
        
        # ì‹ í˜¸ í”Œë¡¯
        ax.plot(time_axis, data.squeeze().numpy(), 'b-', linewidth=1.5)
        
        # ì´ìƒì¹˜ í¬ì¸íŠ¸ í•˜ì´ë¼ì´íŠ¸
        anomaly_mask = point_labels.numpy() > 0.5
        if np.any(anomaly_mask):
            ax.scatter(time_axis[anomaly_mask], data.squeeze().numpy()[anomaly_mask], 
                      color='red', s=20, alpha=0.7)
        
        ax.set_title(f'{anomaly_type}\n(Series: {series_label:.1f})', fontsize=10)
        ax.grid(True, alpha=0.3)
        
        if i >= 5:
            ax.set_xlabel('Time Step')
        if i % 5 == 0:
            ax.set_ylabel('Value')
    
    plt.suptitle(f'{model_name} - Sample Data', fontsize=16)
    plt.tight_layout()
    
    filename = f'samples/{model_name}_samples.png'
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    plt.close()
    
    return filename 

# ============================================================================
# ğŸ‹ï¸ TRAINING AND EVALUATION ENGINE
# ============================================================================

def train_single_model(model, model_name, train_loader, optimizer, device, epochs, scheduler=None):
    """í–¥ìƒëœ ë‹¨ì¼ ëª¨ë¸ í›ˆë ¨ - ì›¹ê²€ìƒ‰ ê¸°ë°˜ ìµœì í™” ì ìš©"""
    print(f"ğŸ”¥ Training {model_name.upper()} for {epochs} epochs...")
    
    model.train()
    total_loss = 0
    num_batches = 0
    best_loss = float('inf')
    patience = 0
    max_patience = 5  # Early stopping patience
    
    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ê°€ì ¸ì˜¤ê¸°
    hp = MODEL_HP.get(model_name, MODEL_HP['fallback'])
    warmup_epochs = hp.get('warmup_epochs', 0)
    gradient_clip = hp.get('gradient_clip', 1.0)
    
    # ì›Œë°ì—… ìŠ¤ì¼€ì¤„ëŸ¬
    if warmup_epochs > 0:
        warmup_scheduler = torch.optim.lr_scheduler.LinearLR(
            optimizer, start_factor=0.1, total_iters=warmup_epochs
        )
    
    for epoch in range(epochs):
        epoch_loss = 0
        model.train()
        
        # ì›Œë°ì—… ë‹¨ê³„
        if epoch < warmup_epochs:
            current_lr = optimizer.param_groups[0]['lr']
            print(f"   Warmup Epoch {epoch+1}/{warmup_epochs}, LR: {current_lr:.6f}")
        
        for batch_idx, (data, point_labels, series_labels, anomaly_types, info) in enumerate(train_loader):
            data = data.to(device)
            
            optimizer.zero_grad()
            
            # ëª¨ë¸ë³„ loss ê³„ì‚° (ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ í˜¸í™˜)
            if hasattr(model, 'compute_loss'):  # ì‹¤ì œ ëª¨ë¸ë“¤
                # ëª¨ë“  ì‹¤ì œ ëª¨ë¸ì€ ê¸°ë³¸ compute_loss ì‚¬ìš©
                loss = model.compute_loss(data)
                    
            else:  # Fallback ëª¨ë¸
                pred_scores, reconstruction = model(data)
                
                # í–¥ìƒëœ ì†ì‹¤ í•¨ìˆ˜
                recon_loss = F.mse_loss(reconstruction, data)
                
                # ì‹œë¦¬ì¦ˆë³„ ì´ìƒ ìŠ¤ì½”ì–´
                series_labels_tensor = series_labels.to(device)
                anomaly_loss = F.binary_cross_entropy_with_logits(pred_scores, series_labels_tensor)
                
                # Point-level ì†ì‹¤ë„ ì¶”ê°€
                point_pred = torch.sigmoid(pred_scores).unsqueeze(1).expand(-1, data.shape[1])
                point_labels_tensor = point_labels.to(device)
                point_loss = F.binary_cross_entropy(point_pred, point_labels_tensor)
                
                # ê°€ì¤‘ í•©ê³„
                loss = 0.4 * recon_loss + 0.4 * anomaly_loss + 0.2 * point_loss
            
            loss.backward()
            
            # í–¥ìƒëœ Gradient clipping
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=gradient_clip)
            
            optimizer.step()
            
            epoch_loss += loss.item()
            total_loss += loss.item()
            num_batches += 1
            
            if batch_idx % 20 == 0:
                current_lr = optimizer.param_groups[0]['lr']
                print(f"   Epoch {epoch+1}/{epochs}, Batch {batch_idx}, Loss: {loss.item():.6f}, LR: {current_lr:.6f}")
        
        avg_epoch_loss = epoch_loss / len(train_loader)
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
        if epoch < warmup_epochs and warmup_epochs > 0:
            warmup_scheduler.step()
        elif scheduler is not None:
            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):
                scheduler.step(avg_epoch_loss)
            else:
                scheduler.step()
        
        # Early stopping
        if avg_epoch_loss < best_loss:
            best_loss = avg_epoch_loss
            patience = 0
        else:
            patience += 1
            
        print(f"   Epoch {epoch+1} completed. Avg Loss: {avg_epoch_loss:.6f}, Best: {best_loss:.6f}, Patience: {patience}")
        
        # Early stopping ì²´í¬
        if patience >= max_patience and epoch > epochs // 2:  # ìµœì†Œ ì ˆë°˜ì€ í›ˆë ¨
            print(f"   Early stopping triggered at epoch {epoch+1}")
            break
    
    avg_total_loss = total_loss / num_batches
    print(f"âœ… {model_name.upper()} training completed. Avg Loss: {avg_total_loss:.6f}, Best Loss: {best_loss:.6f}")
    
    return avg_total_loss

def evaluate_model_comprehensive(model, model_name, test_loader, device, threshold=0.5):
    """í¬ê´„ì  ëª¨ë¸ í‰ê°€"""
    print(f"ğŸ“Š Evaluating {model_name.upper()}...")
    
    model.eval()
    
    # ê²°ê³¼ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸ë“¤
    all_data = []
    all_point_true = []
    all_series_true = []
    all_point_pred = []
    all_series_pred = []
    all_anomaly_types = []
    all_anomaly_info = []
    
    with torch.no_grad():
        for data, point_labels, series_labels, anomaly_types, info in test_loader:
            data = data.to(device)
            
            # ëª¨ë¸ ì˜ˆì¸¡
            series_scores, point_scores = model.detect_anomalies(data)
            
            # CPUë¡œ ì´ë™ ë° numpy ë³€í™˜
            data_np = data.cpu().numpy()
            point_true_np = point_labels.numpy()
            series_true_np = series_labels.numpy()
            point_pred_np = point_scores.cpu().numpy()
            series_pred_np = series_scores.cpu().numpy()
            
            # ë°°ì¹˜ ë°ì´í„° ì €ì¥
            all_data.extend(data_np)
            all_point_true.extend(point_true_np)
            all_series_true.extend(series_true_np)
            all_point_pred.extend(point_pred_np)
            all_series_pred.extend(series_pred_np)
            all_anomaly_types.extend(anomaly_types)
            all_anomaly_info.extend(info)
    
    # Numpy ë°°ì—´ë¡œ ë³€í™˜
    all_data = np.array(all_data)
    all_point_true = np.array(all_point_true)
    all_series_true = np.array(all_series_true)
    all_point_pred = np.array(all_point_pred)
    all_series_pred = np.array(all_series_pred)
    
    # ë©”íŠ¸ë¦­ ê³„ì‚°
    series_metrics = calculate_detailed_metrics(all_series_true, all_series_pred, threshold)
    
    # Point-level ë©”íŠ¸ë¦­ (í‰í‰í•˜ê²Œ ë§Œë“¤ì–´ì„œ ê³„ì‚°)
    point_true_flat = all_point_true.flatten()
    point_pred_flat = all_point_pred.flatten()
    point_metrics = calculate_detailed_metrics(point_true_flat, point_pred_flat, threshold)
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"ğŸ¯ {model_name.upper()} Results:")
    print(f"   Series Level - Acc: {series_metrics['accuracy']:.3f}, F1: {series_metrics['f1']:.3f}, AUC: {series_metrics['auc']:.3f}")
    print(f"   Point Level  - Acc: {point_metrics['accuracy']:.3f}, F1: {point_metrics['f1']:.3f}, AUC: {point_metrics['auc']:.3f}")
    
    # ìƒì„¸ í”Œë¡¯ ìƒì„± (ìƒ˜í”Œë³„)
    create_detailed_plots_by_classification(
        all_data, all_point_true, all_point_pred, all_series_true, all_series_pred,
        all_anomaly_types, threshold, model_name
    )
    
    # Confusion Matrix ìƒì„±
    create_confusion_matrix_plot(all_series_true, all_series_pred, model_name, 'Series', 'confusion_matrices')
    create_confusion_matrix_plot(point_true_flat, point_pred_flat, model_name, 'Point', 'confusion_matrices')
    
    return {
        'series_metrics': series_metrics,
        'point_metrics': point_metrics,
        'predictions': {
            'data': all_data,
            'point_true': all_point_true,
            'series_true': all_series_true,
            'point_pred': all_point_pred,
            'series_pred': all_series_pred,
            'anomaly_types': all_anomaly_types
        }
    }

def create_detailed_plots_by_classification(data, point_true, point_pred, series_true, series_pred, 
                                          anomaly_types, threshold, model_name, max_per_category=5):
    """ë¶„ë¥˜ë³„ ìƒì„¸ í”Œë¡¯ ìƒì„±"""
    if not MATPLOTLIB_AVAILABLE:
        return
        
    # ì˜ˆì¸¡ ê²°ê³¼ì— ë”°ë¥¸ ë¶„ë¥˜
    series_pred_binary = (series_pred > threshold).astype(int)
    series_true_binary = (series_true > 0.5).astype(int)
    
    # 4ê°€ì§€ ì¹´í…Œê³ ë¦¬
    tp_indices = np.where((series_true_binary == 1) & (series_pred_binary == 1))[0]  # True Positive
    tn_indices = np.where((series_true_binary == 0) & (series_pred_binary == 0))[0]  # True Negative
    fp_indices = np.where((series_true_binary == 0) & (series_pred_binary == 1))[0]  # False Positive
    fn_indices = np.where((series_true_binary == 1) & (series_pred_binary == 0))[0]  # False Negative
    
    categories = {
        'true_positive': tp_indices,
        'true_negative': tn_indices,
        'false_positive': fp_indices,
        'false_negative': fn_indices
    }
    
    print(f"ğŸ“ˆ Creating detailed plots for {model_name}:")
    for category, indices in categories.items():
        print(f"   {category}: {len(indices)} samples")
        
        # ê° ì¹´í…Œê³ ë¦¬ì—ì„œ ìµœëŒ€ max_per_categoryê°œ ìƒ˜í”Œ ì €ì¥
        selected_indices = indices[:max_per_category] if len(indices) > max_per_category else indices
        
        for i, idx in enumerate(selected_indices):
            save_dir = f'plots/{model_name}/{category}'
            
            create_detailed_plot(
                data=data[idx].squeeze(),
                point_true=point_true[idx],
                point_pred=point_pred[idx],
                series_true=series_true[idx],
                series_pred=series_pred[idx],
                threshold=threshold,
                model_name=model_name,
                sample_idx=idx,
                anomaly_type=anomaly_types[idx],
                save_dir=save_dir
            )

def create_summary_metrics_plot(all_results):
    """ê°œì„ ëœ ì „ì²´ ëª¨ë¸ ì„±ëŠ¥ ìš”ì•½ í”Œë¡¯ - ì‚¬ìš©ì ìš”ì²­ì‚¬í•­ ë°˜ì˜"""
    if not MATPLOTLIB_AVAILABLE:
        return
        
    models = list(all_results.keys())
    
    # Series-level ë©”íŠ¸ë¦­
    series_acc = [all_results[m]['series_metrics']['accuracy'] for m in models]
    series_f1 = [all_results[m]['series_metrics']['f1'] for m in models]
    series_auc = [all_results[m]['series_metrics']['auc'] for m in models]
    
    # Point-level ë©”íŠ¸ë¦­
    point_acc = [all_results[m]['point_metrics']['accuracy'] for m in models]
    point_f1 = [all_results[m]['point_metrics']['f1'] for m in models]
    point_auc = [all_results[m]['point_metrics']['auc'] for m in models]
    
    # ë¹¨ê°„ìƒ‰-íŒŒë€ìƒ‰ ì»¬ëŸ¬ë§µ ìƒì„± (ë†’ì€ê°’=ë¹¨ê°„ìƒ‰, ë‚®ì€ê°’=íŒŒë€ìƒ‰)
    def get_color_from_value(values, cmap='RdBu_r'):
        """ê°’ì— ë”°ë¥¸ ìƒ‰ìƒ ìƒì„± (ë†’ì€ê°’=ë¹¨ê°„ìƒ‰, ë‚®ì€ê°’=íŒŒë€ìƒ‰)"""
        import matplotlib.cm as cm
        normalized = [(v - min(values)) / (max(values) - min(values)) if max(values) != min(values) else 0.5 for v in values]
        colors = [cm.get_cmap(cmap)(norm) for norm in normalized]
        return colors
    
    # í”Œë¡¯ ìƒì„± (í¬ê¸° ì¦ê°€)
    fig, axes = plt.subplots(2, 3, figsize=(24, 16))
    
    metrics_data = [
        (series_acc, 'Series Accuracy', axes[0, 0]),
        (series_f1, 'Series F1-Score', axes[0, 1]),
        (series_auc, 'Series AUC', axes[0, 2]),
        (point_acc, 'Point Accuracy', axes[1, 0]),
        (point_f1, 'Point F1-Score', axes[1, 1]),
        (point_auc, 'Point AUC', axes[1, 2])
    ]
    
    for data, title, ax in metrics_data:
        # í˜„ì¬ ê°’ ë²”ìœ„ì— ë§ì¶° Yì¶• ìŠ¤ì¼€ì¼ ì¡°ì • (0ì´ ë§¨ ì•„ë˜ì— ê³ ì •ë˜ì§€ ì•Šê²Œ)
        min_val = min(data)
        max_val = max(data)
        margin = (max_val - min_val) * 0.1 if max_val != min_val else 0.1
        y_min = max(0, min_val - margin)  # 0 ì´í•˜ë¡œëŠ” ê°€ì§€ ì•Šë˜, ì—¬ë°± ì¶”ê°€
        y_max = min(1, max_val + margin)  # 1 ì´ìƒìœ¼ë¡œëŠ” ê°€ì§€ ì•Šë˜, ì—¬ë°± ì¶”ê°€
        
        # ë†’ì€ê°’=ë¹¨ê°„ìƒ‰, ë‚®ì€ê°’=íŒŒë€ìƒ‰
        colors = get_color_from_value(data, 'RdBu_r')
        
        bars = ax.bar(models, data, alpha=0.8, color=colors, edgecolor='black', linewidth=2)
        ax.set_title(title, fontsize=20, fontweight='bold', pad=20)
        ax.set_ylim(y_min, y_max)  # í˜„ì¬ ê°’ì— ë§ì¶˜ ìŠ¤ì¼€ì¼
        ax.tick_params(axis='x', rotation=45, labelsize=16)
        ax.tick_params(axis='y', labelsize=16)
        ax.set_ylabel('Score', fontsize=18, fontweight='bold')
        
        # ê°’ í‘œì‹œ (ê¸€ì í¬ê¸° 2ë°°)
        for bar, value in zip(bars, data):
            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + margin*0.3,
                   f'{value:.3f}', ha='center', va='bottom', fontsize=16, fontweight='bold')
        
        # ê²©ì ê°œì„ 
        ax.grid(True, alpha=0.3, linewidth=1)
    
    plt.suptitle('Model Performance Comparison', fontsize=28, fontweight='bold', y=0.98)
    plt.tight_layout()
    
    # final_performance_comparison.png
    filename1 = 'metrics/final_performance_comparison.png'
    plt.savefig(filename1, dpi=300, bbox_inches='tight', facecolor='white')
    
    # ì¶”ê°€ë¡œ performance_bars.pngë„ ìƒì„±
    filename2 = 'metrics/performance_bars.png'
    plt.savefig(filename2, dpi=300, bbox_inches='tight', facecolor='white')
    
    plt.close()
    
    # enhanced_quick_metrics.png ìƒì„± (ë¹¨ê°„ìƒ‰=ë†’ì€ê°’, íŒŒë€ìƒ‰=ë‚®ì€ê°’)
    create_enhanced_quick_metrics(all_results)
    
    print(f"ğŸ“Š Summary metrics saved to {filename1} and {filename2}")
    return filename1

def create_enhanced_quick_metrics(all_results):
    """ê°œì„ ëœ ë¹ ë¥¸ ë©”íŠ¸ë¦­ íˆíŠ¸ë§µ - ë¹¨ê°„ìƒ‰(ë†’ì€ê°’), íŒŒë€ìƒ‰(ë‚®ì€ê°’)"""
    if not MATPLOTLIB_AVAILABLE:
        return
        
    models = list(all_results.keys())
    
    # ë©”íŠ¸ë¦­ ë°ì´í„° ìˆ˜ì§‘
    metrics_matrix = []
    metric_names = ['Series Acc', 'Series F1', 'Series AUC', 'Point Acc', 'Point F1', 'Point AUC']
    
    for model in models:
        row = [
            all_results[model]['series_metrics']['accuracy'],
            all_results[model]['series_metrics']['f1'],
            all_results[model]['series_metrics']['auc'],
            all_results[model]['point_metrics']['accuracy'],
            all_results[model]['point_metrics']['f1'],
            all_results[model]['point_metrics']['auc']
        ]
        metrics_matrix.append(row)
    
    metrics_matrix = np.array(metrics_matrix)
    
    # íˆíŠ¸ë§µ ìƒì„±
    fig, ax = plt.subplots(figsize=(14, 10))
    
    # ë¹¨ê°„ìƒ‰=ë†’ì€ê°’, íŒŒë€ìƒ‰=ë‚®ì€ê°’ (RdBu_r ì»¬ëŸ¬ë§µ)
    im = ax.imshow(metrics_matrix, cmap='RdBu_r', aspect='auto', vmin=0, vmax=1)
    
    # ì¶• ì„¤ì •
    ax.set_xticks(np.arange(len(metric_names)))
    ax.set_yticks(np.arange(len(models)))
    ax.set_xticklabels(metric_names, fontsize=18, fontweight='bold')
    ax.set_yticklabels(models, fontsize=18, fontweight='bold')
    
    # ì œëª©
    ax.set_title('Model Performance Heatmap\n(Red=High, Blue=Low)', 
                fontsize=24, fontweight='bold', pad=30)
    
    # ê°’ í‘œì‹œ
    for i in range(len(models)):
        for j in range(len(metric_names)):
            value = metrics_matrix[i, j]
            text_color = 'white' if value > 0.5 else 'black'
            ax.text(j, i, f'{value:.3f}', ha='center', va='center',
                   color=text_color, fontsize=16, fontweight='bold')
    
    # ì»¬ëŸ¬ë°”
    cbar = plt.colorbar(im, ax=ax, shrink=0.8)
    cbar.set_label('Performance Score', fontsize=18, fontweight='bold')
    cbar.ax.tick_params(labelsize=16)
    
    plt.tight_layout()
    
    filename = 'metrics/enhanced_quick_metrics.png'
    plt.savefig(filename, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    print(f"ğŸ“Š Enhanced quick metrics saved to {filename}")
    return filename

# ============================================================================
# ğŸš€ MAIN EXECUTION
# ============================================================================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ Starting Final Complete Anomaly Detection System")
    print(f"ğŸ“± Device: {GPU_CONFIG['device']}")
    print(f"ğŸ”§ Configuration: {CONFIG}")
    
    # Set random seeds
    torch.manual_seed(CONFIG['SEED'])
    np.random.seed(CONFIG['SEED'])
    
    # Create datasets
    print("\nğŸ“Š Creating datasets...")
    train_dataset = DifficultAnomalyDataset('train', CONFIG['DATA_SIZE'], 'hard')
    test_dataset = DifficultAnomalyDataset('test', CONFIG['DATA_SIZE']//4, 'hard')
    
    # Create data loaders
    train_loader = DataLoader(train_dataset, batch_size=CONFIG['BATCH_SIZE'], shuffle=True, num_workers=0)
    test_loader = DataLoader(test_dataset, batch_size=CONFIG['BATCH_SIZE'], shuffle=False, num_workers=0)
    
    print(f"âœ… Datasets created: Train {len(train_dataset)}, Test {len(test_dataset)}")
    
    # Save sample data
    save_sample_data(test_dataset, 'final_system', 10)
    
    # Create all models
    print("\nğŸ¤– Creating models...")
    models = create_all_models()
    
    # Training and evaluation
    all_results = {}
    
    for model_name, model in models.items():
        print(f"\n{'='*60}")
        print(f"ğŸ”¥ Processing {model_name.upper()}")
        print(f"{'='*60}")
        
        # Skip ensemble models for training (they don't need training) 
        if 'ensemble' not in model_name:
            # í–¥ìƒëœ optimizer ë° scheduler ìƒì„±
            hp = MODEL_HP.get(model_name, MODEL_HP['fallback'])
            
            # ëª¨ë¸ë³„ ìµœì í™”ëœ optimizer
            if model_name in ['tracegpt', 'patchtrad']:
                # Transformer ëª¨ë¸ë“¤ì€ AdamW + ì›œì—… + ì½”ì‚¬ì¸ ìŠ¤ì¼€ì¤„ëŸ¬
                optimizer = torch.optim.AdamW(
                    model.parameters(), 
                    lr=hp['lr'], 
                    weight_decay=hp.get('weight_decay', 1e-4),
                    betas=(0.9, 0.98),  # Transformerì— ìµœì í™”
                    eps=1e-6
                )
                
                # ì½”ì‚¬ì¸ ì–´ë‹ë§ ìŠ¤ì¼€ì¤„ëŸ¬
                scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
                    optimizer, 
                    T_max=hp['epochs'], 
                    eta_min=hp['lr'] * 0.01
                )
                
            elif model_name == 'carla':
                # CARLAëŠ” contrastive learningì— ìµœì í™”
                optimizer = torch.optim.AdamW(
                    model.parameters(),
                    lr=hp['lr'],
                    weight_decay=hp.get('weight_decay', 1e-4),
                    betas=(0.9, 0.999)
                )
                
                # ReduceLROnPlateau ìŠ¤ì¼€ì¤„ëŸ¬ (ì„±ëŠ¥ ê¸°ë°˜)
                scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
                    optimizer,
                    mode='min',
                    factor=0.5,
                    patience=3,
                    min_lr=hp['lr'] * 0.001
                )
                
            elif model_name == 'prodiffad':
                # Diffusion ëª¨ë¸ì€ ë” ì•ˆì •ì ì¸ í•™ìŠµë¥ 
                optimizer = torch.optim.AdamW(
                    model.parameters(),
                    lr=hp['lr'],
                    weight_decay=hp.get('weight_decay', 1e-5),
                    betas=(0.9, 0.999)
                )
                
                # ë‹¨ê³„ë³„ ê°ì†Œ ìŠ¤ì¼€ì¤„ëŸ¬
                scheduler = torch.optim.lr_scheduler.StepLR(
                    optimizer,
                    step_size=hp['epochs'] // 3,
                    gamma=0.5
                )
                
            else:  # fallback
                # ê¸°ë³¸ ì„¤ì •
                optimizer = torch.optim.AdamW(
                    model.parameters(),
                    lr=hp['lr'],
                    weight_decay=hp.get('weight_decay', 1e-4)
                )
                
                scheduler_type = hp.get('scheduler', 'plateau')
                if scheduler_type == 'step':
                    scheduler = torch.optim.lr_scheduler.StepLR(
                        optimizer,
                        step_size=hp.get('step_size', 10),
                        gamma=hp.get('gamma', 0.5)
                    )
                else:
                    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
                        optimizer,
                        mode='min',
                        factor=0.5,
                        patience=3
                    )
            
            print(f"ğŸ”§ Optimizer: {type(optimizer).__name__}, Scheduler: {type(scheduler).__name__}")
            
            # í–¥ìƒëœ í›ˆë ¨
            train_loss = train_single_model(
                model, model_name, train_loader, optimizer, 
                GPU_CONFIG['device'], hp['epochs'], scheduler
            )
            
            # ëª¨ë¸ ì €ì¥ (state_dict + optimizer + scheduler)
            checkpoint = {
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
                'epoch': hp['epochs'],
                'loss': train_loss,
                'hyperparameters': hp
            }
            torch.save(checkpoint, f'pre_trained/{model_name}_final.pth')
            print(f"ğŸ’¾ Model checkpoint saved to pre_trained/{model_name}_final.pth")
        
        # Evaluate model
        results = evaluate_model_comprehensive(
            model, model_name, test_loader, GPU_CONFIG['device'], CONFIG['THRESHOLD']
        )
        
        all_results[model_name] = results
        
        print(f"âœ… {model_name.upper()} processing completed")
    
    # Create summary plots
    print("\nğŸ“Š Creating summary visualizations...")
    create_summary_metrics_plot(all_results)
    
    # Print final summary
    print(f"\n{'='*80}")
    print("ğŸ¯ FINAL RESULTS SUMMARY")
    print(f"{'='*80}")
    
    for model_name, results in all_results.items():
        series_acc = results['series_metrics']['accuracy']
        point_acc = results['point_metrics']['accuracy']
        series_f1 = results['series_metrics']['f1']
        point_f1 = results['point_metrics']['f1']
        
        print(f"{model_name.upper():25} | Series: Acc={series_acc:.3f} F1={series_f1:.3f} | Point: Acc={point_acc:.3f} F1={point_f1:.3f}")
    
    print(f"\nğŸ‰ Complete system execution finished!")
    print(f"ğŸ“ Check outputs in: plots/, metrics/, confusion_matrices/, samples/, pre_trained/")
    
    return all_results

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Final Complete Anomaly Detection System')
    parser.add_argument('--model', type=str, default='all', 
                       choices=['all'] + MODEL_NAMES,
                       help='Model to run (default: all)')
    parser.add_argument('--epochs', type=int, default=None,
                       help='Override epochs for all models')
    parser.add_argument('--batch-size', type=int, default=None,
                       help='Override batch size')
    parser.add_argument('--data-size', type=int, default=None,
                       help='Override dataset size')
    parser.add_argument('--threshold', type=float, default=0.5,
                       help='Anomaly detection threshold')
    parser.add_argument('--difficulty', type=str, default='hard',
                       choices=['easy', 'medium', 'hard'],
                       help='Dataset difficulty level')
    
    args = parser.parse_args()
    
    # Update config with command line arguments
    if args.epochs:
        for model_hp in MODEL_HP.values():
            if 'epochs' in model_hp:
                model_hp['epochs'] = args.epochs
    
    if args.batch_size:
        CONFIG['BATCH_SIZE'] = args.batch_size
    
    if args.data_size:
        CONFIG['DATA_SIZE'] = args.data_size
    
    CONFIG['THRESHOLD'] = args.threshold
    
    # Run main function
    try:
        results = main()
        print("\nâœ… All operations completed successfully!")
    except Exception as e:
        print(f"\nâŒ Error occurred: {e}")
        import traceback
        traceback.print_exc() 