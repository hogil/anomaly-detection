#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Final Complete Anomaly Detection System
- Real models from models folder
- Enhanced difficulty data generation
- Model-specific folders with detailed plots
- Point/Series level metrics and confusion matrices
- Performance optimizations
- Auto GPU/DDP detection
"""

import os
import sys
import argparse
import time
import math
import warnings
import numpy as np
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.distributed as dist
import torch.multiprocessing as mp
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import Dataset, DataLoader, DistributedSampler
from torch.utils.data.distributed import DistributedSampler

# Visualization
try:
    import matplotlib
    matplotlib.use('Agg')
    import matplotlib.pyplot as plt
    import seaborn as sns
    MATPLOTLIB_AVAILABLE = True
    print("âœ… matplotlib available")
except ImportError:
    MATPLOTLIB_AVAILABLE = False
    print("âŒ matplotlib not available")

# Metrics
try:
    from sklearn.metrics import (confusion_matrix, roc_auc_score, accuracy_score, 
                                precision_score, recall_score, f1_score, roc_curve)
    SKLEARN_AVAILABLE = True
    print("âœ… sklearn available")
except ImportError:
    SKLEARN_AVAILABLE = False
    print("âŒ sklearn not available")

warnings.filterwarnings('ignore')

# ============================================================================
# ğŸ”§ GLOBAL CONFIGURATION
# ============================================================================

# GPU Environment Detection
def detect_gpu_environment():
    """GPU í™˜ê²½ ìë™ ê°ì§€ ë° ì„¤ì •"""
    if not torch.cuda.is_available():
        return {'device': 'cpu', 'world_size': 1, 'use_ddp': False}
    
    gpu_count = torch.cuda.device_count()
    print(f"ğŸ”§ GPU ê°ì§€: {gpu_count}ê°œ GPU ë°œê²¬")
    
    if gpu_count == 1:
        return {
            'device': 'cuda:0',
            'world_size': 1,
            'use_ddp': False,
            'gpu_count': gpu_count
        }
    else:
        return {
            'device': 'cuda',
            'world_size': gpu_count,
            'use_ddp': True,
            'gpu_count': gpu_count
        }

GPU_CONFIG = detect_gpu_environment()
print(f"ğŸš€ GPU Configuration: {GPU_CONFIG}")

# Configuration
CONFIG = {
    'DATA_SIZE': 1000,
    'SEQ_LEN': 64,
    'BATCH_SIZE': 16 if not GPU_CONFIG['use_ddp'] else 8,  # Adjust for DDP
    'EPOCHS': 5,
    'LEARNING_RATE': 1e-4,
    'THRESHOLD': 0.5,
    'SEED': 42,
    'WORKERS': 4
}

# Create directories
DIRS = ['samples', 'plots', 'metrics', 'confusion_matrices', 'pre_trained']
for base_dir in DIRS:
    os.makedirs(base_dir, exist_ok=True)
    
# Model-specific plot directories
MODEL_NAMES = ['carla', 'tracegpt', 'patchtrad', 'prodiffad', 
               'patch_trace_ensemble', 'transfer_learning_ensemble', 'multi_model_ensemble']

for model_name in MODEL_NAMES:
    for subdir in ['true_positive', 'true_negative', 'false_positive', 'false_negative']:
        os.makedirs(f'plots/{model_name}/{subdir}', exist_ok=True)

print(f"ğŸ“ Created {len(MODEL_NAMES)} model directories with 4 subdirectories each")

# ============================================================================
# ğŸ¯ ENHANCED DIFFICULT DATASET
# ============================================================================

class DifficultAnomalyDataset(Dataset):
    """í–¥ìƒëœ ë‚œì´ë„ì˜ ì´ìƒ íƒì§€ ë°ì´í„°ì…‹ - Normalì— ê°€ê¹Œìš´ ì´ìƒ íŒ¨í„´"""
    
    def __init__(self, mode='train', size=None, difficulty='hard'):
        np.random.seed(CONFIG['SEED'] if mode == 'train' else CONFIG['SEED'] + 1)
        
        self.mode = mode
        self.difficulty = difficulty
        data_size = size if size else (CONFIG['DATA_SIZE'] if mode == 'train' else CONFIG['DATA_SIZE'] // 4)
        
        print(f"ğŸ“Š Generating {mode} dataset: {data_size} samples (difficulty: {difficulty})")
        
        self.data = []
        self.point_labels = []
        self.series_labels = []
        self.anomaly_types = []
        self.anomaly_info = []  # ìƒì„¸ ì •ë³´
        
        # ë‚œì´ë„ë³„ ì„¤ì •
        if difficulty == 'easy':
            self.noise_std = 0.05
            self.anomaly_strength = (2.0, 4.0)
        elif difficulty == 'medium':
            self.noise_std = 0.08
            self.anomaly_strength = (1.2, 2.5)
        else:  # hard
            self.noise_std = 0.12
            self.anomaly_strength = (0.8, 1.5)  # Normalì— ë§¤ìš° ê°€ê¹Œì›€
        
        for idx in range(data_size):
            series, point_label, series_label, atype, info = self._generate_difficult_series(idx)
            self.data.append(series)
            self.point_labels.append(point_label)
            self.series_labels.append(series_label)
            self.anomaly_types.append(atype)
            self.anomaly_info.append(info)
        
        # í†µê³„ ì¶œë ¥
        normal_count = sum(1 for x in self.series_labels if x == 0)
        anomaly_count = len(self.series_labels) - normal_count
        
        type_counts = {}
        for atype in self.anomaly_types:
            type_counts[atype] = type_counts.get(atype, 0) + 1
        
        print(f"âœ… {mode} dataset complete: Normal {normal_count}, Anomaly {anomaly_count}")
        print(f"   ğŸ“Š Type distribution: {type_counts}")
    
    def _generate_difficult_series(self, idx):
        """Normalì— ê°€ê¹Œìš´ ì–´ë ¤ìš´ ì´ìƒ íŒ¨í„´ ìƒì„±"""
        series = np.random.normal(0, self.noise_std, CONFIG['SEQ_LEN']).astype(np.float32)
        point_label = np.zeros(CONFIG['SEQ_LEN'], dtype=np.float32)
        
        anomaly_type = idx % 6  # 6ê°€ì§€ íƒ€ì…
        
        if anomaly_type == 0:
            # Normal
            series_label = 0.0
            type_name = 'Normal'
            info = {'type': 'normal', 'start': 0, 'end': 0, 'magnitude': 0.0, 'count': 0}
            
        elif anomaly_type == 1:
            # Subtle Spike - ë§¤ìš° ì•½í•œ ìŠ¤íŒŒì´í¬
            n_spikes = np.random.randint(1, 3)
            spike_positions = []
            for _ in range(n_spikes):
                pos = np.random.randint(10, CONFIG['SEQ_LEN']-10)
                # ì•½í•œ ìŠ¤íŒŒì´í¬ (ê¸°ì¡´ ëŒ€ë¹„ 1/3 ê°•ë„)
                magnitude = np.random.uniform(*self.anomaly_strength) * np.random.choice([-1, 1])
                series[pos] += magnitude
                point_label[pos] = 1.0
                
                # ì£¼ë³€ ì˜í–¥ë„ ì¤„ì„
                for offset in [-1, 1]:
                    if 0 <= pos + offset < CONFIG['SEQ_LEN']:
                        series[pos + offset] += magnitude * 0.15  # ê¸°ì¡´ 0.3ì—ì„œ 0.15ë¡œ
                        point_label[pos + offset] = 1.0
                
                spike_positions.append(pos)
            
            series_label = 1.0
            type_name = 'Subtle_Spike'
            info = {'type': 'spike', 'start': spike_positions[0] if spike_positions else 0, 'end': spike_positions[-1] if spike_positions else 0, 'magnitude': magnitude, 'count': n_spikes}
            
        elif anomaly_type == 2:
            # Gradual Mean Shift - ì ì§„ì  ë³€í™”
            start = np.random.randint(15, CONFIG['SEQ_LEN']-25)
            end = start + np.random.randint(15, 25)
            
            # ì ì§„ì  ë³€í™” (ê¸‰ê²©í•˜ì§€ ì•ŠìŒ)
            shift_magnitude = np.random.uniform(*self.anomaly_strength) * np.random.choice([-1, 1])
            transition_length = min(8, (end - start) // 3)
            
            # ì‹œì‘ ë¶€ë¶„ ì ì§„ì  ì¦ê°€
            for i in range(transition_length):
                alpha = i / transition_length
                series[start + i] += shift_magnitude * alpha
                point_label[start + i] = alpha  # ê·¸ë¼ë°ì´ì…˜ ë¼ë²¨
            
            # ì¤‘ê°„ ë¶€ë¶„ ì¼ì •
            mid_start = start + transition_length
            mid_end = end - transition_length
            if mid_end > mid_start:
                series[mid_start:mid_end] += shift_magnitude
                point_label[mid_start:mid_end] = 1.0
            
            # ë ë¶€ë¶„ ì ì§„ì  ê°ì†Œ
            for i in range(transition_length):
                alpha = 1 - (i / transition_length)
                series[end - transition_length + i] += shift_magnitude * alpha
                point_label[end - transition_length + i] = alpha
            
            series_label = 1.0
            type_name = 'Gradual_Shift'
            info = {'type': 'shift', 'start': start, 'end': end, 'magnitude': shift_magnitude, 'count': 1}
            
        elif anomaly_type == 3:
            # Subtle Variance Change - ë¯¸ë¬˜í•œ ë¶„ì‚° ë³€í™”
            start = np.random.randint(10, CONFIG['SEQ_LEN']-20)
            end = start + np.random.randint(15, 25)
            
            # ê¸°ì¡´ ë…¸ì´ì¦ˆ ëŒ€ë¹„ 1.5-2.5ë°° (ê¸°ì¡´ 3-6ë°°ì—ì„œ ì¤„ì„)
            new_std = self.noise_std * np.random.uniform(1.5, 2.5)
            length = end - start
            if length > 0:
                series[start:end] = np.random.normal(0, new_std, length)
                point_label[start:end] = 1.0
            
            series_label = 1.0
            type_name = 'Subtle_Variance'
            info = {'type': 'variance', 'start': start, 'end': end, 'magnitude': new_std, 'count': 1}
            
        elif anomaly_type == 4:
            # Slow Trend - ì²œì²œíˆ ë³€í•˜ëŠ” íŠ¸ë Œë“œ
            start = np.random.randint(5, CONFIG['SEQ_LEN']//3)
            end = start + np.random.randint(CONFIG['SEQ_LEN']//3, CONFIG['SEQ_LEN']//2)
            end = min(end, CONFIG['SEQ_LEN'])
            
            # ì•½í•œ íŠ¸ë Œë“œ
            trend_magnitude = np.random.uniform(*self.anomaly_strength) * np.random.choice([-1, 1])
            length = end - start
            if length > 0:
                trend = np.linspace(0, trend_magnitude, length)
                series[start:end] += trend
                
                # íŠ¸ë Œë“œ ê°•ë„ì— ë”°ë¥¸ ë¼ë²¨ (ì„ í˜• ì¦ê°€)
                trend_labels = np.linspace(0.3, 1.0, length)
                point_label[start:end] = trend_labels
            
            series_label = 1.0
            type_name = 'Slow_Trend'
            info = {'type': 'trend', 'start': start, 'end': end, 'magnitude': trend_magnitude, 'count': 1}
            
        else:  # anomaly_type == 5
            # Complex Pattern - ë³µí•© íŒ¨í„´ (ë§¤ìš° ì–´ë ¤ì›€)
            # ì‘ì€ ìŠ¤íŒŒì´í¬ + ë¯¸ë¬˜í•œ íŠ¸ë Œë“œ
            
            # ì‘ì€ ìŠ¤íŒŒì´í¬
            spike_pos = np.random.randint(10, CONFIG['SEQ_LEN']//2)
            spike_mag = np.random.uniform(*self.anomaly_strength) * 0.7  # ë” ì•½í•˜ê²Œ
            series[spike_pos] += spike_mag
            point_label[spike_pos] = 0.8
            
            # ë¯¸ë¬˜í•œ íŠ¸ë Œë“œ
            trend_start = CONFIG['SEQ_LEN']//2
            trend_end = CONFIG['SEQ_LEN'] - 5
            trend_mag = np.random.uniform(*self.anomaly_strength) * 0.5 * np.random.choice([-1, 1])
            trend_length = trend_end - trend_start
            if trend_length > 0:
                trend = np.linspace(0, trend_mag, trend_length)
                series[trend_start:trend_end] += trend
                point_label[trend_start:trend_end] = np.linspace(0.3, 0.7, trend_length)
            
            series_label = 1.0
            type_name = 'Complex_Pattern'
            info = {'type': 'complex', 'start': spike_pos, 'end': trend_end, 'magnitude': spike_mag, 'count': 2}
        
        return series, point_label, series_label, type_name, info
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        return (
            torch.from_numpy(self.data[idx]).unsqueeze(-1),  # [seq_len, 1]
            torch.from_numpy(self.point_labels[idx]),        # [seq_len]
            torch.tensor(self.series_labels[idx], dtype=torch.float32),  # scalar
            self.anomaly_types[idx],                         # string
            self.anomaly_info[idx]                          # dict
        ) 

# ============================================================================
# ğŸ¤– MODEL IMPORTS AND HYPERPARAMETERS
# ============================================================================

def safe_model_import():
    """ì•ˆì „í•œ ëª¨ë¸ import ë° fallback ì²˜ë¦¬"""
    models = {}
    
    # 1. CARLA ëª¨ë¸
    try:
        sys.path.append(os.path.join(os.path.dirname(__file__), 'models/carla'))
        from models.carla.model import create_carla_model
        models['carla'] = create_carla_model
        print("âœ… CARLA model imported successfully")
    except Exception as e:
        print(f"âŒ CARLA import failed: {e}")
        models['carla'] = None
    
    # 2. TraceGPT ëª¨ë¸
    try:
        sys.path.append(os.path.join(os.path.dirname(__file__), 'models/tracegpt'))
        from models.tracegpt.model import create_tracegpt_model
        models['tracegpt'] = create_tracegpt_model
        print("âœ… TraceGPT model imported successfully")
    except Exception as e:
        print(f"âŒ TraceGPT import failed: {e}")
        models['tracegpt'] = None
    
    # 3. PatchTrAD ëª¨ë¸
    try:
        sys.path.append(os.path.join(os.path.dirname(__file__), 'models/patchtrad'))
        from models.patchtrad.model import create_patchtrad_model
        models['patchtrad'] = create_patchtrad_model
        print("âœ… PatchTrAD model imported successfully")
    except Exception as e:
        print(f"âŒ PatchTrAD import failed: {e}")
        models['patchtrad'] = None
    
    # 4. ProDiffAD ëª¨ë¸
    try:
        sys.path.append(os.path.join(os.path.dirname(__file__), 'models/prodiffad'))
        from models.prodiffad.model import create_prodiffad_model
        models['prodiffad'] = create_prodiffad_model
        print("âœ… ProDiffAD model imported successfully")
    except Exception as e:
        print(f"âŒ ProDiffAD import failed: {e}")
        models['prodiffad'] = None
    
    return models

# Fallback ë‹¨ìˆœ ëª¨ë¸
class FallbackAnomalyModel(nn.Module):
    def __init__(self, seq_len=64, hidden_dim=128):
        super().__init__()
        self.encoder = nn.LSTM(1, hidden_dim//2, 2, batch_first=True, bidirectional=True)
        self.decoder = nn.Linear(hidden_dim, 1)
        self.reconstructor = nn.Linear(hidden_dim, seq_len)
        
    def forward(self, x):
        lstm_out, _ = self.encoder(x)
        anomaly_scores = torch.sigmoid(self.decoder(lstm_out[:, -1, :]))
        reconstruction = self.reconstructor(lstm_out[:, -1, :]).unsqueeze(-1)
        return anomaly_scores.squeeze(), reconstruction.expand_as(x)
    
    def detect_anomalies(self, x):
        with torch.no_grad():
            anomaly_scores, reconstruction = self.forward(x)
            recon_error = torch.mean((x - reconstruction) ** 2, dim=[1, 2])
            point_scores = torch.mean((x - reconstruction) ** 2, dim=2)
        return recon_error, point_scores

# ëª¨ë¸ë³„ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°
MODEL_HP = {
    'carla': {
        'seq_len': CONFIG['SEQ_LEN'],
        'hidden_dim': 256,
        'encoder_layers': 3,
        'temperature': 0.07,  # ë” ë‚®ì€ ì˜¨ë„ë¡œ ë” ì§‘ì¤‘ì  í•™ìŠµ
        'margin': 1.2,        # ë” í° ë§ˆì§„ìœ¼ë¡œ êµ¬ë¶„ë ¥ í–¥ìƒ
        'dropout': 0.1,
        'lr': 1e-4,
        'epochs': 8
    },
    'tracegpt': {
        'seq_len': CONFIG['SEQ_LEN'],
        'd_model': 256,
        'n_heads': 8,
        'n_layers': 8,        # ë” ê¹Šì€ ë„¤íŠ¸ì›Œí¬
        'd_ff': 1024,
        'dropout': 0.1,
        'lr': 5e-5,           # ë” ë‚®ì€ í•™ìŠµë¥ 
        'epochs': 10
    },
    'patchtrad': {
        'seq_len': CONFIG['SEQ_LEN'],
        'patch_size': 8,      # ë” í° íŒ¨ì¹˜ë¡œ ê¸€ë¡œë²Œ íŒ¨í„´ í¬ì°©
        'stride': 4,
        'd_model': 256,
        'n_heads': 8,
        'n_layers': 8,
        'd_ff': 1024,
        'dropout': 0.1,
        'lr': 1e-4,
        'epochs': 8
    },
    'prodiffad': {
        'seq_len': CONFIG['SEQ_LEN'],
        'hidden_dim': 256,
        'num_layers': 6,
        'dropout': 0.1,
        'lr': 1e-4,
        'epochs': 12         # Diffusionì€ ë” ë§ì€ epochs í•„ìš”
    },
    'fallback': {
        'seq_len': CONFIG['SEQ_LEN'],
        'hidden_dim': 128,
        'lr': 1e-3,
        'epochs': 5
    }
}

# ì•™ìƒë¸” ëª¨ë¸ë“¤
class PatchTraceEnsemble(nn.Module):
    """PatchTrAD + TraceGPT ì•™ìƒë¸”"""
    def __init__(self, models_dict):
        super().__init__()
        self.patch_model = models_dict.get('patchtrad')
        self.trace_model = models_dict.get('tracegpt')
        self.weight = nn.Parameter(torch.tensor([0.6, 0.4]))  # ê°€ì¤‘ì¹˜ í•™ìŠµ
        
    def detect_anomalies(self, x):
        if self.patch_model and self.trace_model:
            with torch.no_grad():
                patch_series, patch_point = self.patch_model.detect_anomalies(x)
                trace_series, trace_point = self.trace_model.detect_anomalies(x)
                
                weights = torch.softmax(self.weight, dim=0)
                combined_series = weights[0] * patch_series + weights[1] * trace_series
                combined_point = weights[0] * patch_point + weights[1] * trace_point
                
                return combined_series, combined_point
        return torch.zeros(x.shape[0]), torch.zeros(x.shape[0], x.shape[1])

class TransferLearningEnsemble(nn.Module):
    """ì „ì´í•™ìŠµ ê¸°ë°˜ ì•™ìƒë¸”"""
    def __init__(self, models_dict):
        super().__init__()
        self.carla_model = models_dict.get('carla')
        self.trace_model = models_dict.get('tracegpt')
        
    def detect_anomalies(self, x):
        if self.carla_model and self.trace_model:
            with torch.no_grad():
                carla_series, carla_point = self.carla_model.detect_anomalies(x)
                trace_series, trace_point = self.trace_model.detect_anomalies(x)
                
                # CARLAì˜ representation learning í™œìš©
                combined_series = 0.7 * carla_series + 0.3 * trace_series
                combined_point = 0.7 * carla_point + 0.3 * trace_point
                
                return combined_series, combined_point
        return torch.zeros(x.shape[0]), torch.zeros(x.shape[1])

class MultiModelEnsemble(nn.Module):
    """ì „ì²´ ëª¨ë¸ ì•™ìƒë¸”"""
    def __init__(self, models_dict):
        super().__init__()
        self.models = {k: v for k, v in models_dict.items() if v is not None}
        self.weights = nn.Parameter(torch.ones(len(self.models)) / len(self.models))
        
    def detect_anomalies(self, x):
        if not self.models:
            return torch.zeros(x.shape[0]), torch.zeros(x.shape[0], x.shape[1])
            
        with torch.no_grad():
            series_scores = []
            point_scores = []
            
            for model in self.models.values():
                series, point = model.detect_anomalies(x)
                series_scores.append(series)
                point_scores.append(point)
            
            # ê°€ì¤‘í‰ê· 
            weights = torch.softmax(self.weights, dim=0)
            combined_series = sum(w * s for w, s in zip(weights, series_scores))
            combined_point = sum(w * p for w, p in zip(weights, point_scores))
            
            return combined_series, combined_point

# ëª¨ë¸ ìƒì„± í•¨ìˆ˜
def create_all_models():
    """ëª¨ë“  ëª¨ë¸ ìƒì„± ë° ì„¤ì •"""
    print("ğŸš€ Creating all models...")
    
    # Import models
    imported_models = safe_model_import()
    models = {}
    
    # ê°œë³„ ëª¨ë¸ë“¤
    for name, create_func in imported_models.items():
        # ëª¨ë¸ ìƒì„±ìš© í•˜ì´í¼íŒŒë¼ë¯¸í„° (lr, epochs ì œì™¸)
        model_params = {k: v for k, v in MODEL_HP[name].items() if k not in ['lr', 'epochs']}
        fallback_params = {k: v for k, v in MODEL_HP['fallback'].items() if k not in ['lr', 'epochs']}
        
        if create_func is not None:
            try:
                model = create_func(**model_params)
                if GPU_CONFIG['device'] != 'cpu':
                    model = model.to(GPU_CONFIG['device'])
                models[name] = model
                print(f"âœ… {name.upper()} model created successfully")
            except Exception as e:
                print(f"âŒ {name.upper()} model creation failed: {e}")
                # Fallback ëª¨ë¸ ì‚¬ìš©
                model = FallbackAnomalyModel(**fallback_params)
                if GPU_CONFIG['device'] != 'cpu':
                    model = model.to(GPU_CONFIG['device'])
                models[name] = model
                print(f"ğŸ”„ Using fallback model for {name.upper()}")
        else:
            # Fallback ëª¨ë¸
            model = FallbackAnomalyModel(**fallback_params)
            if GPU_CONFIG['device'] != 'cpu':
                model = model.to(GPU_CONFIG['device'])
            models[name] = model
            print(f"ğŸ”„ Using fallback model for {name.upper()}")
    
    # ì•™ìƒë¸” ëª¨ë¸ë“¤
    models['patch_trace_ensemble'] = PatchTraceEnsemble(models)
    models['transfer_learning_ensemble'] = TransferLearningEnsemble(models)
    models['multi_model_ensemble'] = MultiModelEnsemble(models)
    
    if GPU_CONFIG['device'] != 'cpu':
        for name in ['patch_trace_ensemble', 'transfer_learning_ensemble', 'multi_model_ensemble']:
            models[name] = models[name].to(GPU_CONFIG['device'])
    
    print(f"ğŸ¯ Total {len(models)} models created")
    return models 

# ============================================================================
# ğŸ“Š DETAILED VISUALIZATION AND EVALUATION
# ============================================================================

def create_detailed_plot(data, point_true, point_pred, series_true, series_pred, 
                        threshold, model_name, sample_idx, anomaly_type, save_dir):
    """ìƒì„¸í•œ ê°œë³„ ì‹œê³„ì—´ í”Œë¡¯ ìƒì„±"""
    if not MATPLOTLIB_AVAILABLE:
        return
        
    fig, axes = plt.subplots(3, 1, figsize=(15, 12))
    
    # Time axis
    time_axis = np.arange(len(data))
    
    # 1. Original Signal with Anomalies
    axes[0].plot(time_axis, data, 'b-', linewidth=1.5, label='Signal', alpha=0.8)
    
    # True anomalies
    true_anomaly_mask = point_true > 0.5
    if np.any(true_anomaly_mask):
        axes[0].scatter(time_axis[true_anomaly_mask], data[true_anomaly_mask], 
                       color='red', s=30, label='True Anomalies', alpha=0.7, zorder=5)
    
    # Predicted anomalies
    pred_anomaly_mask = point_pred > threshold
    if np.any(pred_anomaly_mask):
        axes[0].scatter(time_axis[pred_anomaly_mask], data[pred_anomaly_mask], 
                       color='orange', s=20, marker='x', label='Pred Anomalies', alpha=0.7, zorder=4)
    
    axes[0].set_title(f'{model_name} - Sample {sample_idx} ({anomaly_type})\n'
                     f'Series: True={series_true:.1f}, Pred={series_pred:.3f}', fontsize=12)
    axes[0].set_ylabel('Signal Value')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    # 2. Anomaly Scores
    axes[1].plot(time_axis, point_pred, 'g-', linewidth=2, label='Anomaly Score')
    axes[1].axhline(y=threshold, color='red', linestyle='--', linewidth=2, label=f'Threshold ({threshold:.3f})')
    
    # Fill anomaly zones
    axes[1].fill_between(time_axis, 0, point_pred, 
                        where=(point_pred > threshold), 
                        color='red', alpha=0.2, label='Anomaly Zone')
    
    axes[1].set_ylabel('Anomaly Score')
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)
    axes[1].set_ylim(0, 1)
    
    # 3. True vs Predicted Comparison
    axes[2].plot(time_axis, point_true, 'r-', linewidth=2, label='True Labels', alpha=0.8)
    axes[2].plot(time_axis, point_pred, 'g-', linewidth=2, label='Pred Scores', alpha=0.8)
    axes[2].axhline(y=threshold, color='black', linestyle='--', linewidth=1, label=f'Threshold')
    
    axes[2].set_xlabel('Time Step')
    axes[2].set_ylabel('Score/Label')
    axes[2].legend()
    axes[2].grid(True, alpha=0.3)
    axes[2].set_ylim(0, 1)
    
    plt.tight_layout()
    
    # Save plot
    os.makedirs(save_dir, exist_ok=True)
    filename = f'{save_dir}/sample_{sample_idx}_{anomaly_type}.png'
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    plt.close()
    
    return filename

def create_confusion_matrix_plot(true_labels, pred_labels, model_name, level, save_dir):
    """í˜¼ë™ í–‰ë ¬ ìƒì„±"""
    if not SKLEARN_AVAILABLE or not MATPLOTLIB_AVAILABLE:
        return None
        
    # ì´ì§„ ë¶„ë¥˜ë¡œ ë³€í™˜
    true_binary = (true_labels > 0.5).astype(int)
    pred_binary = (pred_labels > 0.5).astype(int)
    
    cm = confusion_matrix(true_binary, pred_binary)
    
    fig, ax = plt.subplots(figsize=(8, 6))
    im = ax.imshow(cm, interpolation='nearest', cmap='Blues')
    ax.figure.colorbar(im, ax=ax)
    
    # í´ë˜ìŠ¤ ì´ë¦„
    classes = ['Normal', 'Anomaly']
    ax.set(xticks=np.arange(cm.shape[1]),
           yticks=np.arange(cm.shape[0]),
           xticklabels=classes, yticklabels=classes,
           title=f'{model_name} - {level} Level Confusion Matrix',
           ylabel='True Label',
           xlabel='Predicted Label')
    
    # í…ìŠ¤íŠ¸ ì£¼ì„
    thresh = cm.max() / 2.
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j, i, format(cm[i, j], 'd'),
                   ha="center", va="center",
                   color="white" if cm[i, j] > thresh else "black")
    
    plt.tight_layout()
    
    os.makedirs(save_dir, exist_ok=True)
    filename = f'{save_dir}/{model_name}_{level}_confusion_matrix.png'
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    plt.close()
    
    return filename

def calculate_detailed_metrics(true_labels, pred_scores, threshold=0.5):
    """ìƒì„¸ ë©”íŠ¸ë¦­ ê³„ì‚°"""
    if not SKLEARN_AVAILABLE:
        return {}
        
    # ì´ì§„ ì˜ˆì¸¡ê°’
    pred_binary = (pred_scores > threshold).astype(int)
    true_binary = (true_labels > 0.5).astype(int)
    
    metrics = {
        'accuracy': accuracy_score(true_binary, pred_binary),
        'precision': precision_score(true_binary, pred_binary, zero_division='warn'),
        'recall': recall_score(true_binary, pred_binary, zero_division='warn'),
        'f1': f1_score(true_binary, pred_binary, zero_division='warn'),
        'threshold': threshold
    }
    
    # AUC ê³„ì‚° (ê°€ëŠ¥í•œ ê²½ìš°)
    try:
        metrics['auc'] = roc_auc_score(true_binary, pred_scores)
    except:
        metrics['auc'] = 0.0
        
    return metrics

def save_sample_data(dataset, model_name, num_samples=10):
    """ìƒ˜í”Œ ë°ì´í„° ì €ì¥"""
    if not MATPLOTLIB_AVAILABLE:
        return
        
    fig, axes = plt.subplots(2, 5, figsize=(20, 8))
    axes = axes.flatten()
    
    for i in range(min(num_samples, len(dataset))):
        data, point_labels, series_label, anomaly_type, info = dataset[i]
        
        ax = axes[i]
        time_axis = np.arange(len(data))
        
        # ì‹ í˜¸ í”Œë¡¯
        ax.plot(time_axis, data.squeeze().numpy(), 'b-', linewidth=1.5)
        
        # ì´ìƒì¹˜ í¬ì¸íŠ¸ í•˜ì´ë¼ì´íŠ¸
        anomaly_mask = point_labels.numpy() > 0.5
        if np.any(anomaly_mask):
            ax.scatter(time_axis[anomaly_mask], data.squeeze().numpy()[anomaly_mask], 
                      color='red', s=20, alpha=0.7)
        
        ax.set_title(f'{anomaly_type}\n(Series: {series_label:.1f})', fontsize=10)
        ax.grid(True, alpha=0.3)
        
        if i >= 5:
            ax.set_xlabel('Time Step')
        if i % 5 == 0:
            ax.set_ylabel('Value')
    
    plt.suptitle(f'{model_name} - Sample Data', fontsize=16)
    plt.tight_layout()
    
    filename = f'samples/{model_name}_samples.png'
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    plt.close()
    
    return filename 

# ============================================================================
# ğŸ‹ï¸ TRAINING AND EVALUATION ENGINE
# ============================================================================

def train_single_model(model, model_name, train_loader, optimizer, device, epochs):
    """ë‹¨ì¼ ëª¨ë¸ í›ˆë ¨"""
    print(f"ğŸ”¥ Training {model_name.upper()} for {epochs} epochs...")
    
    model.train()
    total_loss = 0
    num_batches = 0
    
    for epoch in range(epochs):
        epoch_loss = 0
        for batch_idx, (data, point_labels, series_labels, anomaly_types, info) in enumerate(train_loader):
            data = data.to(device)
            
            optimizer.zero_grad()
            
            # ëª¨ë¸ë³„ loss ê³„ì‚°
            if hasattr(model, 'compute_loss'):  # CARLA, TraceGPT, PatchTrAD
                loss = model.compute_loss(data)
            else:  # Fallback ëª¨ë¸
                pred_scores, reconstruction = model(data)
                # ì¬êµ¬ì„± ì†ì‹¤ + ì´ìƒ íƒì§€ ì†ì‹¤
                recon_loss = F.mse_loss(reconstruction, data)
                
                # ì‹œë¦¬ì¦ˆë³„ ì´ìƒ ìŠ¤ì½”ì–´ ìƒì„±
                series_labels_tensor = series_labels.to(device)
                anomaly_loss = F.binary_cross_entropy(pred_scores, series_labels_tensor)
                
                loss = recon_loss + anomaly_loss
            
            loss.backward()
            
            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            epoch_loss += loss.item()
            total_loss += loss.item()
            num_batches += 1
            
            if batch_idx % 20 == 0:
                print(f"   Epoch {epoch+1}/{epochs}, Batch {batch_idx}, Loss: {loss.item():.6f}")
        
        avg_epoch_loss = epoch_loss / len(train_loader)
        print(f"   Epoch {epoch+1} completed. Avg Loss: {avg_epoch_loss:.6f}")
    
    avg_total_loss = total_loss / num_batches
    print(f"âœ… {model_name.upper()} training completed. Avg Loss: {avg_total_loss:.6f}")
    
    return avg_total_loss

def evaluate_model_comprehensive(model, model_name, test_loader, device, threshold=0.5):
    """í¬ê´„ì  ëª¨ë¸ í‰ê°€"""
    print(f"ğŸ“Š Evaluating {model_name.upper()}...")
    
    model.eval()
    
    # ê²°ê³¼ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸ë“¤
    all_data = []
    all_point_true = []
    all_series_true = []
    all_point_pred = []
    all_series_pred = []
    all_anomaly_types = []
    all_anomaly_info = []
    
    with torch.no_grad():
        for data, point_labels, series_labels, anomaly_types, info in test_loader:
            data = data.to(device)
            
            # ëª¨ë¸ ì˜ˆì¸¡
            series_scores, point_scores = model.detect_anomalies(data)
            
            # CPUë¡œ ì´ë™ ë° numpy ë³€í™˜
            data_np = data.cpu().numpy()
            point_true_np = point_labels.numpy()
            series_true_np = series_labels.numpy()
            point_pred_np = point_scores.cpu().numpy()
            series_pred_np = series_scores.cpu().numpy()
            
            # ë°°ì¹˜ ë°ì´í„° ì €ì¥
            all_data.extend(data_np)
            all_point_true.extend(point_true_np)
            all_series_true.extend(series_true_np)
            all_point_pred.extend(point_pred_np)
            all_series_pred.extend(series_pred_np)
            all_anomaly_types.extend(anomaly_types)
            all_anomaly_info.extend(info)
    
    # Numpy ë°°ì—´ë¡œ ë³€í™˜
    all_data = np.array(all_data)
    all_point_true = np.array(all_point_true)
    all_series_true = np.array(all_series_true)
    all_point_pred = np.array(all_point_pred)
    all_series_pred = np.array(all_series_pred)
    
    # ë©”íŠ¸ë¦­ ê³„ì‚°
    series_metrics = calculate_detailed_metrics(all_series_true, all_series_pred, threshold)
    
    # Point-level ë©”íŠ¸ë¦­ (í‰í‰í•˜ê²Œ ë§Œë“¤ì–´ì„œ ê³„ì‚°)
    point_true_flat = all_point_true.flatten()
    point_pred_flat = all_point_pred.flatten()
    point_metrics = calculate_detailed_metrics(point_true_flat, point_pred_flat, threshold)
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"ğŸ¯ {model_name.upper()} Results:")
    print(f"   Series Level - Acc: {series_metrics['accuracy']:.3f}, F1: {series_metrics['f1']:.3f}, AUC: {series_metrics['auc']:.3f}")
    print(f"   Point Level  - Acc: {point_metrics['accuracy']:.3f}, F1: {point_metrics['f1']:.3f}, AUC: {point_metrics['auc']:.3f}")
    
    # ìƒì„¸ í”Œë¡¯ ìƒì„± (ìƒ˜í”Œë³„)
    create_detailed_plots_by_classification(
        all_data, all_point_true, all_point_pred, all_series_true, all_series_pred,
        all_anomaly_types, threshold, model_name
    )
    
    # Confusion Matrix ìƒì„±
    create_confusion_matrix_plot(all_series_true, all_series_pred, model_name, 'Series', 'confusion_matrices')
    create_confusion_matrix_plot(point_true_flat, point_pred_flat, model_name, 'Point', 'confusion_matrices')
    
    return {
        'series_metrics': series_metrics,
        'point_metrics': point_metrics,
        'predictions': {
            'data': all_data,
            'point_true': all_point_true,
            'series_true': all_series_true,
            'point_pred': all_point_pred,
            'series_pred': all_series_pred,
            'anomaly_types': all_anomaly_types
        }
    }

def create_detailed_plots_by_classification(data, point_true, point_pred, series_true, series_pred, 
                                          anomaly_types, threshold, model_name, max_per_category=5):
    """ë¶„ë¥˜ë³„ ìƒì„¸ í”Œë¡¯ ìƒì„±"""
    if not MATPLOTLIB_AVAILABLE:
        return
        
    # ì˜ˆì¸¡ ê²°ê³¼ì— ë”°ë¥¸ ë¶„ë¥˜
    series_pred_binary = (series_pred > threshold).astype(int)
    series_true_binary = (series_true > 0.5).astype(int)
    
    # 4ê°€ì§€ ì¹´í…Œê³ ë¦¬
    tp_indices = np.where((series_true_binary == 1) & (series_pred_binary == 1))[0]  # True Positive
    tn_indices = np.where((series_true_binary == 0) & (series_pred_binary == 0))[0]  # True Negative
    fp_indices = np.where((series_true_binary == 0) & (series_pred_binary == 1))[0]  # False Positive
    fn_indices = np.where((series_true_binary == 1) & (series_pred_binary == 0))[0]  # False Negative
    
    categories = {
        'true_positive': tp_indices,
        'true_negative': tn_indices,
        'false_positive': fp_indices,
        'false_negative': fn_indices
    }
    
    print(f"ğŸ“ˆ Creating detailed plots for {model_name}:")
    for category, indices in categories.items():
        print(f"   {category}: {len(indices)} samples")
        
        # ê° ì¹´í…Œê³ ë¦¬ì—ì„œ ìµœëŒ€ max_per_categoryê°œ ìƒ˜í”Œ ì €ì¥
        selected_indices = indices[:max_per_category] if len(indices) > max_per_category else indices
        
        for i, idx in enumerate(selected_indices):
            save_dir = f'plots/{model_name}/{category}'
            
            create_detailed_plot(
                data=data[idx].squeeze(),
                point_true=point_true[idx],
                point_pred=point_pred[idx],
                series_true=series_true[idx],
                series_pred=series_pred[idx],
                threshold=threshold,
                model_name=model_name,
                sample_idx=idx,
                anomaly_type=anomaly_types[idx],
                save_dir=save_dir
            )

def create_summary_metrics_plot(all_results):
    """ì „ì²´ ëª¨ë¸ ì„±ëŠ¥ ìš”ì•½ í”Œë¡¯"""
    if not MATPLOTLIB_AVAILABLE:
        return
        
    models = list(all_results.keys())
    
    # Series-level ë©”íŠ¸ë¦­
    series_acc = [all_results[m]['series_metrics']['accuracy'] for m in models]
    series_f1 = [all_results[m]['series_metrics']['f1'] for m in models]
    series_auc = [all_results[m]['series_metrics']['auc'] for m in models]
    
    # Point-level ë©”íŠ¸ë¦­
    point_acc = [all_results[m]['point_metrics']['accuracy'] for m in models]
    point_f1 = [all_results[m]['point_metrics']['f1'] for m in models]
    point_auc = [all_results[m]['point_metrics']['auc'] for m in models]
    
    # í”Œë¡¯ ìƒì„±
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    
    metrics_data = [
        (series_acc, 'Series Accuracy', axes[0, 0]),
        (series_f1, 'Series F1-Score', axes[0, 1]),
        (series_auc, 'Series AUC', axes[0, 2]),
        (point_acc, 'Point Accuracy', axes[1, 0]),
        (point_f1, 'Point F1-Score', axes[1, 1]),
        (point_auc, 'Point AUC', axes[1, 2])
    ]
    
    for data, title, ax in metrics_data:
        bars = ax.bar(models, data, alpha=0.7)
        ax.set_title(title, fontsize=14)
        ax.set_ylim(0, 1)
        ax.tick_params(axis='x', rotation=45)
        
        # ê°’ í‘œì‹œ
        for bar, value in zip(bars, data):
            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                   f'{value:.3f}', ha='center', va='bottom', fontsize=10)
    
    plt.suptitle('Model Performance Comparison', fontsize=16)
    plt.tight_layout()
    
    filename = 'metrics/final_performance_comparison.png'
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"ğŸ“Š Summary metrics saved to {filename}")
    return filename

# ============================================================================
# ğŸš€ MAIN EXECUTION
# ============================================================================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ Starting Final Complete Anomaly Detection System")
    print(f"ğŸ“± Device: {GPU_CONFIG['device']}")
    print(f"ğŸ”§ Configuration: {CONFIG}")
    
    # Set random seeds
    torch.manual_seed(CONFIG['SEED'])
    np.random.seed(CONFIG['SEED'])
    
    # Create datasets
    print("\nğŸ“Š Creating datasets...")
    train_dataset = DifficultAnomalyDataset('train', CONFIG['DATA_SIZE'], 'hard')
    test_dataset = DifficultAnomalyDataset('test', CONFIG['DATA_SIZE']//4, 'hard')
    
    # Create data loaders
    train_loader = DataLoader(train_dataset, batch_size=CONFIG['BATCH_SIZE'], shuffle=True, num_workers=0)
    test_loader = DataLoader(test_dataset, batch_size=CONFIG['BATCH_SIZE'], shuffle=False, num_workers=0)
    
    print(f"âœ… Datasets created: Train {len(train_dataset)}, Test {len(test_dataset)}")
    
    # Save sample data
    save_sample_data(test_dataset, 'final_system', 10)
    
    # Create all models
    print("\nğŸ¤– Creating models...")
    models = create_all_models()
    
    # Training and evaluation
    all_results = {}
    
    for model_name, model in models.items():
        print(f"\n{'='*60}")
        print(f"ğŸ”¥ Processing {model_name.upper()}")
        print(f"{'='*60}")
        
        # Skip ensemble models for training (they don't need training)
        if 'ensemble' not in model_name:
            # Create optimizer
            hp = MODEL_HP.get(model_name, MODEL_HP['fallback'])
            optimizer = torch.optim.AdamW(model.parameters(), lr=hp['lr'], weight_decay=1e-5)
            
            # Train model
            train_loss = train_single_model(
                model, model_name, train_loader, optimizer, 
                GPU_CONFIG['device'], hp['epochs']
            )
            
            # Save model
            torch.save(model.state_dict(), f'pre_trained/{model_name}_final.pth')
            print(f"ğŸ’¾ Model saved to pre_trained/{model_name}_final.pth")
        
        # Evaluate model
        results = evaluate_model_comprehensive(
            model, model_name, test_loader, GPU_CONFIG['device'], CONFIG['THRESHOLD']
        )
        
        all_results[model_name] = results
        
        print(f"âœ… {model_name.upper()} processing completed")
    
    # Create summary plots
    print("\nğŸ“Š Creating summary visualizations...")
    create_summary_metrics_plot(all_results)
    
    # Print final summary
    print(f"\n{'='*80}")
    print("ğŸ¯ FINAL RESULTS SUMMARY")
    print(f"{'='*80}")
    
    for model_name, results in all_results.items():
        series_acc = results['series_metrics']['accuracy']
        point_acc = results['point_metrics']['accuracy']
        series_f1 = results['series_metrics']['f1']
        point_f1 = results['point_metrics']['f1']
        
        print(f"{model_name.upper():25} | Series: Acc={series_acc:.3f} F1={series_f1:.3f} | Point: Acc={point_acc:.3f} F1={point_f1:.3f}")
    
    print(f"\nğŸ‰ Complete system execution finished!")
    print(f"ğŸ“ Check outputs in: plots/, metrics/, confusion_matrices/, samples/, pre_trained/")
    
    return all_results

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Final Complete Anomaly Detection System')
    parser.add_argument('--model', type=str, default='all', 
                       choices=['all'] + MODEL_NAMES,
                       help='Model to run (default: all)')
    parser.add_argument('--epochs', type=int, default=None,
                       help='Override epochs for all models')
    parser.add_argument('--batch-size', type=int, default=None,
                       help='Override batch size')
    parser.add_argument('--data-size', type=int, default=None,
                       help='Override dataset size')
    parser.add_argument('--threshold', type=float, default=0.5,
                       help='Anomaly detection threshold')
    parser.add_argument('--difficulty', type=str, default='hard',
                       choices=['easy', 'medium', 'hard'],
                       help='Dataset difficulty level')
    
    args = parser.parse_args()
    
    # Update config with command line arguments
    if args.epochs:
        for model_hp in MODEL_HP.values():
            if 'epochs' in model_hp:
                model_hp['epochs'] = args.epochs
    
    if args.batch_size:
        CONFIG['BATCH_SIZE'] = args.batch_size
    
    if args.data_size:
        CONFIG['DATA_SIZE'] = args.data_size
    
    CONFIG['THRESHOLD'] = args.threshold
    
    # Run main function
    try:
        results = main()
        print("\nâœ… All operations completed successfully!")
    except Exception as e:
        print(f"\nâŒ Error occurred: {e}")
        import traceback
        traceback.print_exc() 