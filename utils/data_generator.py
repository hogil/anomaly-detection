#!/usr/bin/env python3
"""
High-Quality Balanced Dataset Generator
ê· í˜•ì¡íŒ ê³ í’ˆì§ˆ ë°ì´í„°ì…‹ ìƒì„±ê¸° (Normal 80%, Anomaly 20%)
"""

import torch
import numpy as np
import matplotlib.pyplot as plt
from typing import Tuple, List

def generate_balanced_dataset(
    n_samples: int = 500,
    seq_len: int = 64,
    normal_ratio: float = 0.8,  # Normal 80%
    noise_level: float = 0.02,  # ë‚®ì€ ë…¸ì´ì¦ˆë¡œ ê³ í’ˆì§ˆ ë°ì´í„°
    random_seed: int = 42
) -> Tuple[torch.Tensor, torch.Tensor, List[str]]:
    """
    ê· í˜•ì¡íŒ ê³ í’ˆì§ˆ ì‹œê³„ì—´ ë°ì´í„°ì…‹ ìƒì„±
    
    Args:
        n_samples: ì´ ìƒ˜í”Œ ìˆ˜ (500ê°œ)
        seq_len: ì‹œê³„ì—´ ê¸¸ì´ (64)
        normal_ratio: ì •ìƒ ë°ì´í„° ë¹„ìœ¨ (0.8 = 80%)
        noise_level: ë…¸ì´ì¦ˆ ìˆ˜ì¤€ (0.02ë¡œ ë‚®ì¶¤)
        random_seed: ì¬í˜„ ê°€ëŠ¥í•œ ì‹œë“œ
    
    Returns:
        data: [n_samples, seq_len, 1] í˜•íƒœì˜ í…ì„œ
        labels: [n_samples] í˜•íƒœì˜ ë¼ë²¨ (0=Normal, 1=Anomaly)
        types: ê° ìƒ˜í”Œì˜ íƒ€ì… ë¦¬ìŠ¤íŠ¸
    """
    np.random.seed(random_seed)
    torch.manual_seed(random_seed)
    
    normal_samples = int(n_samples * normal_ratio)
    anomaly_samples = n_samples - normal_samples
    
    data = []
    labels = []
    types = []
    
    print(f"ğŸ”„ Generating balanced dataset:")
    print(f"   Total: {n_samples} samples")
    print(f"   Normal: {normal_samples} ({normal_ratio*100:.1f}%)")
    print(f"   Anomaly: {anomaly_samples} ({(1-normal_ratio)*100:.1f}%)")
    print(f"   Sequence length: {seq_len}")
    print(f"   Noise level: {noise_level}")
    
    # ==========================================
    # NORMAL DATA GENERATION (ê³ í’ˆì§ˆ ì •ìƒ ë°ì´í„°)
    # ==========================================
    
    for i in range(normal_samples):
        t = np.linspace(0, 1, seq_len)
        
        # 5ê°€ì§€ ë‹¤ì–‘í•œ ì •ìƒ íŒ¨í„´ (ê³ í’ˆì§ˆ)
        pattern_type = i % 5
        
        if pattern_type == 0:
            # ì„ í˜• ì¦ê°€ íŠ¸ë Œë“œ
            base = t * 2.0
            noise = np.random.normal(0, noise_level, seq_len)
            ts = base + noise
            
        elif pattern_type == 1:
            # ì§€ìˆ˜ ì„±ì¥ íŠ¸ë Œë“œ
            base = np.exp(t * 0.8) - 1
            noise = np.random.normal(0, noise_level, seq_len)
            ts = base + noise
            
        elif pattern_type == 2:
            # ë¡œê·¸ ì„±ì¥ íŠ¸ë Œë“œ
            base = np.log(1 + t * 5)
            noise = np.random.normal(0, noise_level, seq_len)
            ts = base + noise
            
        elif pattern_type == 3:
            # 2ì°¨ í•¨ìˆ˜ íŠ¸ë Œë“œ
            base = t ** 2 * 3
            noise = np.random.normal(0, noise_level, seq_len)
            ts = base + noise
            
        else:
            # S-ê³¡ì„  (ì‹œê·¸ëª¨ì´ë“œ)
            base = 2 / (1 + np.exp(-10 * (t - 0.5)))
            noise = np.random.normal(0, noise_level, seq_len)
            ts = base + noise
        
        data.append(ts)
        labels.append(0)
        types.append('Normal')
    
    # ==========================================
    # ANOMALY DATA GENERATION (ëª…í™•í•œ ì´ìƒ ë°ì´í„°)
    # ==========================================
    
    anomaly_per_type = anomaly_samples // 4
    
    # 1. Spike Anomalies (ê°•í•œ ìŠ¤íŒŒì´í¬)
    for i in range(anomaly_per_type):
        # ì •ìƒ ë² ì´ìŠ¤ ìƒì„±
        t = np.linspace(0, 1, seq_len)
        base = t * 2.0
        noise = np.random.normal(0, noise_level, seq_len)
        ts = base + noise
        
        # ê°•í•œ ìŠ¤íŒŒì´í¬ ì¶”ê°€ (2-4ê°œ)
        num_spikes = np.random.randint(1, 4)
        spike_positions = np.random.choice(
            range(seq_len//4, 3*seq_len//4), 
            size=num_spikes, 
            replace=False
        )
        
        for pos in spike_positions:
            # ë§¤ìš° ê°•í•œ ìŠ¤íŒŒì´í¬ (ì •ìƒ ë²”ìœ„ì˜ 5-10ë°°)
            spike_magnitude = np.random.uniform(3.0, 6.0)
            direction = np.random.choice([-1, 1])
            ts[pos] += direction * spike_magnitude
        
        data.append(ts)
        labels.append(1)
        types.append('Spike')
    
    # 2. Mean Shift Anomalies (ìˆ˜ì¤€ ë³€í™”)
    for i in range(anomaly_per_type):
        t = np.linspace(0, 1, seq_len)
        base = t * 2.0
        noise = np.random.normal(0, noise_level, seq_len)
        ts = base + noise
        
        # ê°•í•œ ìˆ˜ì¤€ ë³€í™”
        shift_point = np.random.randint(seq_len//4, 3*seq_len//4)
        shift_magnitude = np.random.uniform(2.0, 4.0)
        direction = np.random.choice([-1, 1])
        
        ts[shift_point:] += direction * shift_magnitude
        
        data.append(ts)
        labels.append(1)
        types.append('Mean_Shift')
    
    # 3. Variance Change Anomalies (ë¶„ì‚° ë³€í™”)
    for i in range(anomaly_per_type):
        t = np.linspace(0, 1, seq_len)
        base = t * 2.0
        ts = base.copy()
        
        # ë³€í™” êµ¬ê°„ ì„¤ì •
        change_start = np.random.randint(seq_len//4, seq_len//2)
        change_end = np.random.randint(change_start + seq_len//8, 3*seq_len//4)
        
        # ì •ìƒ êµ¬ê°„: ë‚®ì€ ë…¸ì´ì¦ˆ
        ts[:change_start] += np.random.normal(0, noise_level, change_start)
        ts[change_end:] += np.random.normal(0, noise_level, seq_len - change_end)
        
        # ì´ìƒ êµ¬ê°„: ë§¤ìš° ë†’ì€ ë¶„ì‚°
        high_variance = noise_level * 15  # 15ë°° ì¦ê°€
        ts[change_start:change_end] += np.random.normal(0, high_variance, change_end - change_start)
        
        data.append(ts)
        labels.append(1)
        types.append('Variance_Change')
    
    # 4. Trend Change Anomalies (íŠ¸ë Œë“œ ë³€í™”)
    remaining = anomaly_samples - 3 * anomaly_per_type
    for i in range(remaining):
        t = np.linspace(0, 1, seq_len)
        base = t * 2.0
        noise = np.random.normal(0, noise_level, seq_len)
        ts = base + noise
        
        # ê¸‰ê²©í•œ íŠ¸ë Œë“œ ë³€í™”
        change_point = np.random.randint(seq_len//4, 3*seq_len//4)
        
        # ìƒˆë¡œìš´ íŠ¸ë Œë“œ ì¶”ê°€ (ë§¤ìš° ê¸‰ê²©í•¨)
        new_trend_slope = np.random.uniform(-4.0, 4.0)
        trend_length = seq_len - change_point
        new_trend = np.linspace(0, new_trend_slope, trend_length)
        
        ts[change_point:] += new_trend
        
        data.append(ts)
        labels.append(1)
        types.append('Trend_Change')
    
    # í…ì„œë¡œ ë³€í™˜
    data = torch.FloatTensor(np.array(data)).unsqueeze(-1)  # [n_samples, seq_len, 1]
    labels = torch.LongTensor(labels)
    
    print(f"âœ… Dataset generated successfully:")
    print(f"   Shape: {data.shape}")
    print(f"   Normal samples: {(labels == 0).sum().item()}")
    print(f"   Anomaly samples: {(labels == 1).sum().item()}")
    print(f"   Class distribution: {(labels == 0).sum().item()/(len(labels))*100:.1f}% Normal, {(labels == 1).sum().item()/(len(labels))*100:.1f}% Anomaly")
    
    return data, labels, types

def save_dataset_samples(data: torch.Tensor, labels: torch.Tensor, types: List[str], save_path: str = "samples/"):
    """ë°ì´í„°ì…‹ ìƒ˜í”Œ ì‹œê°í™” ë° ì €ì¥"""
    import os
    os.makedirs(save_path, exist_ok=True)
    
    fig, axes = plt.subplots(2, 5, figsize=(20, 8))
    fig.suptitle('Balanced High-Quality Dataset Samples', fontsize=16)
    
    # Normal ìƒ˜í”Œ 5ê°œ
    normal_indices = (labels == 0).nonzero().flatten()[:5]
    for i, idx in enumerate(normal_indices):
        axes[0, i].plot(data[idx].squeeze(), 'b-', linewidth=2)
        axes[0, i].set_title(f'Normal #{i+1}')
        axes[0, i].grid(True, alpha=0.3)
    
    # Anomaly ìƒ˜í”Œ 5ê°œ (ê° íƒ€ì…ë³„ë¡œ)
    anomaly_types = ['Spike', 'Mean_Shift', 'Variance_Change', 'Trend_Change']
    for i, anom_type in enumerate(anomaly_types):
        # í•´ë‹¹ íƒ€ì…ì˜ ì²« ë²ˆì§¸ ìƒ˜í”Œ ì°¾ê¸°
        type_indices = [j for j, t in enumerate(types) if t == anom_type]
        if type_indices:
            idx = type_indices[0]
            axes[1, i].plot(data[idx].squeeze(), 'r-', linewidth=2)
            axes[1, i].set_title(f'{anom_type}')
            axes[1, i].grid(True, alpha=0.3)
    
    # ë§ˆì§€ë§‰ subplotì€ ì „ì²´ ë¶„í¬
    axes[1, 4].bar(['Normal', 'Anomaly'], 
                   [(labels == 0).sum().item(), (labels == 1).sum().item()],
                   color=['blue', 'red'], alpha=0.7)
    axes[1, 4].set_title('Class Distribution')
    axes[1, 4].set_ylabel('Count')
    
    plt.tight_layout()
    plt.savefig(f'{save_path}balanced_dataset_samples.png', dpi=150, bbox_inches='tight')
    plt.close()
    
    print(f"âœ… Dataset samples saved to {save_path}balanced_dataset_samples.png")

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    data, labels, types = generate_balanced_dataset(n_samples=500, seq_len=64)
    save_dataset_samples(data, labels, types) 